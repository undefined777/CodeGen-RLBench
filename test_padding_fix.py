#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•paddingä¿®å¤æ˜¯å¦æœ‰æ•ˆ

éªŒè¯right-paddingé…ç½®çš„ä¸€è‡´æ€§
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from pathlib import Path
from optimized_rl_trainer import convert_qwen_examples_to_features, Example


def test_padding_consistency():
    """æµ‹è¯•paddingé…ç½®çš„ä¸€è‡´æ€§"""
    print("ğŸ” æµ‹è¯•paddingé…ç½®ä¸€è‡´æ€§")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    model_path = "/home/cxy/Qwen2.5-Coder/finetuning/sft/checkpoints/qwen0.5b-lr5e-5-wr10-wd0.0-bsz1024-maxlen1280"
    
    print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        padding_side='right'  # ä½¿ç”¨right-padding
    )
    
    # ç¡®ä¿æœ‰pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print(f"âœ… Tokenizeré…ç½®:")
    print(f"   Padding side: {tokenizer.padding_side}")
    print(f"   Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
    print(f"   EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
    
    # æµ‹è¯•åŸºæœ¬paddingåŠŸèƒ½
    print(f"\nğŸ§ª æµ‹è¯•åŸºæœ¬paddingåŠŸèƒ½:")
    test_texts = [
        "Hello world",
        "Translate Java to C++",
        "This is a longer text for testing padding functionality"
    ]
    
    for i, text in enumerate(test_texts):
        print(f"\n   æµ‹è¯• {i+1}: {text}")
        
        # ç¼–ç 
        encoded = tokenizer(
            text, 
            padding=True, 
            truncation=True, 
            max_length=50, 
            return_tensors="pt"
        )
        
        input_ids = encoded['input_ids'][0]
        attention_mask = encoded['attention_mask'][0]
        
        # æ£€æŸ¥paddingä½ç½®
        pad_positions = (input_ids == tokenizer.pad_token_id)
        mask_positions = (attention_mask == 0)
        
        print(f"   è¾“å…¥é•¿åº¦: {len(input_ids)}")
        print(f"   æœ‰æ•ˆé•¿åº¦: {attention_mask.sum().item()}")
        print(f"   Paddingä½ç½®: {pad_positions.sum().item()} tokens")
        
        if torch.equal(pad_positions, mask_positions):
            print(f"   âœ… Paddingä¸attention maskä¸€è‡´")
        else:
            print(f"   âŒ Paddingä¸attention maskä¸ä¸€è‡´ï¼")
            
        # è§£ç éªŒè¯
        decoded = tokenizer.decode(input_ids, skip_special_tokens=True)
        print(f"   è§£ç ç»“æœ: {decoded}")
    
    # æµ‹è¯•ç‰¹å¾æå–å‡½æ•°
    print(f"\nğŸ§ª æµ‹è¯•ç‰¹å¾æå–å‡½æ•°:")
    
    # åˆ›å»ºæµ‹è¯•æ ·æœ¬
    class TempArgs:
        max_source_length = 100
        max_target_length = 100
        source_lang = "java"
        target_lang = "cpp"
    
    test_example = Example(
        source_orig="Translate Java to C++: class Test { }",
        target_orig="Here's the C++ translation:\n\n```cpp\nclass Test { };\n```",
        system_orig="You are a helpful assistant for code translation."
    )
    
    features = convert_qwen_examples_to_features([test_example], tokenizer, TempArgs(), stage='test')
    
    if features:
        feature = features[0]
        print(f"   ç‰¹å¾æå–æˆåŠŸ")
        print(f"   Sourceé•¿åº¦: {len(feature.source_ids)}")
        print(f"   Targeté•¿åº¦: {len(feature.target_ids)}")
        
        # æ£€æŸ¥padding
        source_pad_count = sum(1 for x in feature.source_ids if x == tokenizer.pad_token_id)
        target_pad_count = sum(1 for x in feature.target_ids if x == tokenizer.pad_token_id)
        
        print(f"   Source padding: {source_pad_count} tokens")
        print(f"   Target padding: {target_pad_count} tokens")
        
        # æ£€æŸ¥paddingä½ç½®ï¼ˆåº”è¯¥æ˜¯æœ«å°¾ï¼‰
        source_pad_positions = [i for i, x in enumerate(feature.source_ids) if x == tokenizer.pad_token_id]
        target_pad_positions = [i for i, x in enumerate(feature.target_ids) if x == tokenizer.pad_token_id]
        
        print(f"   Source paddingä½ç½®: {source_pad_positions}")
        print(f"   Target paddingä½ç½®: {target_pad_positions}")
        
        # éªŒè¯right-paddingï¼ˆpaddingåº”è¯¥åœ¨æœ«å°¾ï¼‰
        if source_pad_positions and max(source_pad_positions) == len(feature.source_ids) - 1:
            print(f"   âœ… Sourceä½¿ç”¨right-padding")
        else:
            print(f"   âŒ Source paddingä½ç½®ä¸æ­£ç¡®")
            
        if target_pad_positions and max(target_pad_positions) == len(feature.target_ids) - 1:
            print(f"   âœ… Targetä½¿ç”¨right-padding")
        else:
            print(f"   âŒ Target paddingä½ç½®ä¸æ­£ç¡®")
    else:
        print(f"   âŒ ç‰¹å¾æå–å¤±è´¥")
    
    print(f"\nğŸ‰ Paddingä¸€è‡´æ€§æµ‹è¯•å®Œæˆ")


def test_model_generation():
    """æµ‹è¯•æ¨¡å‹ç”ŸæˆåŠŸèƒ½"""
    print(f"\nğŸ¤– æµ‹è¯•æ¨¡å‹ç”ŸæˆåŠŸèƒ½")
    print("=" * 60)
    
    model_path = "/home/cxy/Qwen2.5-Coder/finetuning/sft/checkpoints/qwen0.5b-lr5e-5-wr10-wd0.0-bsz1024-maxlen1280"
    
    try:
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side='right'
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print(f"âœ… æ¨¡å‹å’ŒtokenizeråŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•ç”Ÿæˆ
        test_prompt = "Translate Java to C++:\n\n```java\nclass Test {\n    public static void main(String[] args) {\n        System.out.println(\"Hello\");\n    }\n}\n```"
        
        print(f"ğŸ“ æµ‹è¯•æç¤º: {test_prompt[:100]}...")
        
        # ç¼–ç 
        inputs = tokenizer(
            test_prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=200
        )
        
        # ç§»åŠ¨åˆ°GPU
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        print(f"ğŸ“Š è¾“å…¥å½¢çŠ¶: {inputs['input_ids'].shape}")
        print(f"ğŸ“Š Attention maskå½¢çŠ¶: {inputs['attention_mask'].shape}")
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # è§£ç 
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"ğŸ¤– ç”Ÿæˆç»“æœ: {generated_text}")
        
        # æ£€æŸ¥ç”Ÿæˆè´¨é‡
        if "class" in generated_text.lower() and "main" in generated_text.lower():
            print(f"âœ… ç”Ÿæˆå†…å®¹çœ‹èµ·æ¥åˆç†")
        else:
            print(f"âš ï¸  ç”Ÿæˆå†…å®¹å¯èƒ½æœ‰é—®é¢˜")
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹paddingä¿®å¤éªŒè¯æµ‹è¯•")
    print("=" * 80)
    
    # æµ‹è¯•paddingä¸€è‡´æ€§
    test_padding_consistency()
    
    # æµ‹è¯•æ¨¡å‹ç”Ÿæˆ
    test_model_generation()
    
    print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("=" * 80) 