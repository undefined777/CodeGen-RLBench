#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è°ƒè¯•è¾“å…¥æ„é€ æ–¹å¼
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from pathlib import Path
from optimized_rl_trainer import convert_qwen_examples_to_features, Example


def debug_input_construction():
    """è°ƒè¯•è¾“å…¥æ„é€ æ–¹å¼"""
    print("ğŸ” è°ƒè¯•è¾“å…¥æ„é€ æ–¹å¼")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    model_path = "/home/cxy/Qwen2.5-Coder/finetuning/sft/checkpoints/qwen0.5b-lr5e-5-wr10-wd0.0-bsz1024-maxlen1280"
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        padding_side='right'
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    # æµ‹è¯•æç¤º
    test_prompt = "Translate Java to C++:\n\n```java\nclass Test {\n    public static void main(String[] args) {\n        System.out.println(\"Hello\");\n    }\n}\n```"
    
    print(f"ğŸ“ æµ‹è¯•æç¤º:")
    print(f"{test_prompt}")
    
    # æ–¹æ³•1: ç›´æ¥ä½¿ç”¨tokenizer (å¿«é€Ÿæµ‹è¯•ä¸­çš„æ–¹æ³•)
    print(f"\nğŸ”§ æ–¹æ³•1: ç›´æ¥ä½¿ç”¨tokenizer")
    print("-" * 40)
    
    inputs1 = tokenizer(
        test_prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=200
    )
    
    inputs1 = {k: v.to(model.device) for k, v in inputs1.items()}
    
    print(f"   è¾“å…¥å½¢çŠ¶: {inputs1['input_ids'].shape}")
    print(f"   Attention maskå½¢çŠ¶: {inputs1['attention_mask'].shape}")
    print(f"   æœ‰æ•ˆé•¿åº¦: {inputs1['attention_mask'].sum().item()}")
    
    # æ£€æŸ¥padding
    pad_positions1 = (inputs1['input_ids'][0] == tokenizer.pad_token_id)
    mask_positions1 = (inputs1['attention_mask'][0] == 0)
    
    print(f"   Padding tokens: {pad_positions1.sum().item()}")
    print(f"   Mask zeros: {mask_positions1.sum().item()}")
    
    if torch.equal(pad_positions1, mask_positions1):
        print(f"   âœ… Paddingä¸attention maskä¸€è‡´")
    else:
        print(f"   âŒ Paddingä¸attention maskä¸ä¸€è‡´ï¼")
    
    # ç”Ÿæˆæµ‹è¯•1
    print(f"\nğŸ¤– ç”Ÿæˆæµ‹è¯•1...")
    with torch.no_grad():
        outputs1 = model.generate(
            **inputs1,
            max_new_tokens=50,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    generated_text1 = tokenizer.decode(outputs1[0], skip_special_tokens=True)
    new_tokens1 = outputs1.shape[1] - inputs1['input_ids'].shape[1]
    
    print(f"   ç”Ÿæˆé•¿åº¦: {new_tokens1} tokens")
    print(f"   ç”Ÿæˆå†…å®¹: {generated_text1}")
    
    # æ–¹æ³•2: ä½¿ç”¨convert_qwen_examples_to_features (ç«¯åˆ°ç«¯æµ‹è¯•ä¸­çš„æ–¹æ³•)
    print(f"\nğŸ”§ æ–¹æ³•2: ä½¿ç”¨convert_qwen_examples_to_features")
    print("-" * 40)
    
    # åˆ›å»ºExampleå¯¹è±¡
    example = Example(
        idx=0,
        source=test_prompt,
        target="",
        source_orig=test_prompt,
        target_orig=""
    )
    
    # åˆ›å»ºargså¯¹è±¡
    class TempArgs:
        max_source_length = 400
        max_target_length = 400
        source_lang = "java"
        target_lang = "cpp"
    
    args = TempArgs()
    
    # ä½¿ç”¨ç‰¹å¾æå–å‡½æ•°
    features = convert_qwen_examples_to_features([example], tokenizer, args, stage='test')
    
    if features:
        feature = features[0]
        
        # è½¬æ¢ä¸ºtensor
        source_ids = torch.tensor(feature.source_ids, dtype=torch.long).unsqueeze(0)
        source_mask = torch.tensor(feature.source_mask, dtype=torch.long).unsqueeze(0)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        source_ids = source_ids.to(model.device)
        source_mask = source_mask.to(model.device)
        
        print(f"   è¾“å…¥å½¢çŠ¶: {source_ids.shape}")
        print(f"   Attention maskå½¢çŠ¶: {source_mask.shape}")
        print(f"   æœ‰æ•ˆé•¿åº¦: {source_mask.sum().item()}")
        
        # æ£€æŸ¥padding
        pad_positions2 = (source_ids[0] == tokenizer.pad_token_id)
        mask_positions2 = (source_mask[0] == 0)
        
        print(f"   Padding tokens: {pad_positions2.sum().item()}")
        print(f"   Mask zeros: {mask_positions2.sum().item()}")
        
        if torch.equal(pad_positions2, mask_positions2):
            print(f"   âœ… Paddingä¸attention maskä¸€è‡´")
        else:
            print(f"   âŒ Paddingä¸attention maskä¸ä¸€è‡´ï¼")
        
        # ç”Ÿæˆæµ‹è¯•2
        print(f"\nğŸ¤– ç”Ÿæˆæµ‹è¯•2...")
        with torch.no_grad():
            outputs2 = model.generate(
                input_ids=source_ids,
                attention_mask=source_mask,
                max_new_tokens=50,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        generated_text2 = tokenizer.decode(outputs2[0], skip_special_tokens=True)
        new_tokens2 = outputs2.shape[1] - source_ids.shape[1]
        
        print(f"   ç”Ÿæˆé•¿åº¦: {new_tokens2} tokens")
        print(f"   ç”Ÿæˆå†…å®¹: {generated_text2}")
        
        # æ¯”è¾ƒä¸¤ç§æ–¹æ³•
        print(f"\nğŸ“Š æ–¹æ³•æ¯”è¾ƒ:")
        print(f"   æ–¹æ³•1ç”Ÿæˆé•¿åº¦: {new_tokens1} tokens")
        print(f"   æ–¹æ³•2ç”Ÿæˆé•¿åº¦: {new_tokens2} tokens")
        
        if new_tokens1 > 1 and new_tokens2 <= 1:
            print(f"   âŒ æ–¹æ³•2æœ‰é—®é¢˜ï¼")
        elif new_tokens1 <= 1 and new_tokens2 > 1:
            print(f"   âŒ æ–¹æ³•1æœ‰é—®é¢˜ï¼")
        elif new_tokens1 > 1 and new_tokens2 > 1:
            print(f"   âœ… ä¸¤ç§æ–¹æ³•éƒ½æ­£å¸¸")
        else:
            print(f"   âŒ ä¸¤ç§æ–¹æ³•éƒ½æœ‰é—®é¢˜ï¼")
    
    print(f"\nğŸ‰ è°ƒè¯•å®Œæˆï¼")


if __name__ == "__main__":
    debug_input_construction() 