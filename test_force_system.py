#!/usr/bin/env python3
"""
æµ‹è¯•å¼ºåˆ¶æ·»åŠ systemæ¶ˆæ¯çš„åŠŸèƒ½
"""

import json
import torch
from transformers import AutoTokenizer
import os
from optimized_rl_trainer import read_qwen_examples, convert_qwen_examples_to_features, Example

def test_force_system():
    """æµ‹è¯•å¼ºåˆ¶æ·»åŠ systemæ¶ˆæ¯"""
    
    # è®¾ç½®æ¨¡å‹è·¯å¾„
    model_path = "/home/cxy/Qwen2.5-Coder/finetuning/sft/checkpoints/qwen0.5b-lr5e-5-wr10-wd0.0-bsz1024-maxlen1280"
    
    print("ğŸ” æµ‹è¯•å¼ºåˆ¶æ·»åŠ systemæ¶ˆæ¯")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    
    try:
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_examples = []
        
        # ç¤ºä¾‹1ï¼šæœ‰systemæ¶ˆæ¯çš„æ•°æ®
        example1 = Example(
            idx=0,
            source="test source code",
            target="test target code", 
            source_orig="Translate this Java code to C++",
            target_orig="Here's the C++ translation"
        )
        setattr(example1, "system_orig", "You are a helpful assistant for code translation.")
        test_examples.append(example1)
        
        # ç¤ºä¾‹2ï¼šæ— systemæ¶ˆæ¯çš„æ•°æ®
        example2 = Example(
            idx=1,
            source="test source code 2",
            target="test target code 2",
            source_orig="Translate this Python code to Java", 
            target_orig="Here's the Java translation"
        )
        # ä¸è®¾ç½®system_origï¼Œæ¨¡æ‹Ÿæ²¡æœ‰systemæ¶ˆæ¯çš„æƒ…å†µ
        test_examples.append(example2)
        
        # åˆ›å»ºç®€å•çš„argså¯¹è±¡
        class TempArgs:
            def __init__(self):
                self.max_source_length = 400
                self.max_target_length = 400
                self.source_lang = "java"
                self.target_lang = "cpp"
        
        args = TempArgs()
        
        print(f"\nğŸ§ª æµ‹è¯•convert_qwen_examples_to_features:")
        
        # è½¬æ¢ç‰¹å¾
        features = convert_qwen_examples_to_features(test_examples, tokenizer, args, stage='train')
        
        print(f"è½¬æ¢äº† {len(features)} ä¸ªç‰¹å¾")
        
        # æ£€æŸ¥æ¯ä¸ªç‰¹å¾
        for i, feature in enumerate(features):
            print(f"\nğŸ“ ç‰¹å¾ {i+1}:")
            
            # è§£ç source_idsæŸ¥çœ‹å†…å®¹
            source_text = tokenizer.decode(feature.source_ids, skip_special_tokens=True)
            print(f"   Sourceæ–‡æœ¬é•¿åº¦: {len(source_text)}")
            print(f"   Sourceæ–‡æœ¬å‰200å­—ç¬¦: {source_text[:200]}...")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«systemå†…å®¹
            has_system = "system" in source_text.lower() or "assistant" in source_text.lower()
            print(f"   åŒ…å«systemå†…å®¹: {has_system}")
            
            if has_system:
                print(f"   âœ… æˆåŠŸæ·»åŠ äº†systemæ¶ˆæ¯")
            else:
                print(f"   âŒ ç¼ºå°‘systemæ¶ˆæ¯")
        
        print(f"\nğŸ¯ æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_force_system() 