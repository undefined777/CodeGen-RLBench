# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02-ppo.ipynb (unless otherwise specified).

__all__ = ['AdaptiveKLController', 'FixedKLController', 'PPOCoderTrainer']

# Cell
import numpy as np
import torch.nn.functional as F
from torch.optim import Adam, AdamW
import torch.optim as optim
import torch
import collections
import time
import random
import logging
from transformers import RobertaTokenizer
from utils import (logprobs_from_logits,
                         whiten,
                         clip_by_value,
                         entropy_from_logits,
                         flatten_dict,
                         average_torch_dicts,
                         stats_to_np,
                         stack_dicts,
                         add_suffix)
from mem import log_mem, mem_guard
from dataclasses import dataclass
from typing import Dict, Tuple, Optional, List


@dataclass
class PPOConfig:
    """PPOcoder configuration parameters"""
    # åŸºç¡€å­¦ä¹ å‚æ•°
    lr: float = 1e-5
    adam_eps: float = 1e-8
    
    # PPO æ ¸å¿ƒå‚æ•°
    gamma: float = 0.99           # Discount factor
    lam: float = 0.95            # GAE lambda
    cliprange: float = 0.2       # PPO clip range
    cliprange_value: float = 0.2  # Value function clip range
    vf_coef: float = 0.5         # Value function loss coefficient
    ppo_epochs: int = 4          # PPO optimization epochs
    
    # æ‰¹å¤„ç†å‚æ•°
    batch_size: int = 48
    minibatch_size: int = 12     # PPO minibatch size
    forward_batch_size: int = 16
    
    # æ¢¯åº¦ç´¯ç§¯å‚æ•°
    gradient_accumulation_steps: int = 1  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œå¢åŠ æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
    
    # ğŸ”§ New: é˜¶æ®µæ€§è®­ç»ƒå‚æ•°
    critic_warmup_steps: int = 0  # criticé¢„çƒ­æ­¥æ•°ï¼Œåœ¨æ­¤æœŸé—´åªè®­ç»ƒcriticï¼Œ0è¡¨ç¤ºä¸é¢„çƒ­
    
    # KL æ§åˆ¶å™¨å‚æ•°
    init_kl_coef: float = 0.05    # Initial KL coefficient
    kl_target: float = 1       # KL target value
    adaptive_kl: bool = True     # Whether to use adaptive KL control
    horizon: int = 10000         # Time window for KL controller adjustment
    
    # è®¾å¤‡é…ç½®
    device: str = "cuda"
    
    # Tokenizer
    tokenizer: RobertaTokenizer = None


class AdaptiveKLController:
    """Adaptive KL divergence controller, dynamically adjusting KL penalty coefficient"""
    
    def __init__(self, init_kl_coef: float, target: float, horizon: int = 10000):
        self.value = init_kl_coef
        self.target = target
        self.horizon = horizon

    def update(self, current_kl: float, n_steps: int) -> None:
        """Update coefficient based on current KL divergence"""
        proportional_error = np.clip(current_kl / self.target - 1, -0.2, 0.2)
        mult = 1 + proportional_error * n_steps / self.horizon
        self.value *= mult

        
class FixedKLController:
    """Fixed KL divergence controller"""
    
    def __init__(self, kl_coef: float):
        self.value = kl_coef

    def update(self, current_kl: float, n_steps: int) -> None:
        """Fixed coefficient, no update"""
        pass


class PPOCoderTrainer:
    """
    PPOCoder trainer: Deep reinforcement learning for code generation based on execution feedback
    
    Core features:
    1. Multi-dimensional rewards from compiler feedback, AST matching, and DFG matching
    2. Adaptive KL divergence control to prevent divergence from reference model
    3. PPO algorithm specifically optimized for code generation tasks
    4. Supports Qwen2.5-Coder and other large code models
    """

    def __init__(self, model, ref_model, config: Optional[PPOConfig] = None, **kwargs):
        """
        Initialize PPOCoder trainer
        
        Args:
            model: Training policy model (Actor + Critic)
            ref_model: Reference model, used to calculate KL divergence
            config: PPO configuration parameters
        """
        self.config = config or PPOConfig()
        
        # Backward compatibility: map old parameter names to new parameter names
        param_mapping = {
            'adap_kl_ctrl': 'adaptive_kl',
            'target': 'kl_target', 
            'horizon': 'horizon'  # If needed
        }
        
        # Update configuration parameters
        for key, value in kwargs.items():
            # Check if the parameter name needs to be mapped
            mapped_key = param_mapping.get(key, key)
            if hasattr(self.config, mapped_key):
                setattr(self.config, mapped_key, value)
            elif hasattr(self.config, key):
                setattr(self.config, key, value)
        
        self.model = model
        self.ref_model = ref_model
        
        # Optimizer settings
        self.optimizer = AdamW(
            model.parameters(), 
            lr=self.config.lr, 
            eps=self.config.adam_eps
        )
        
        # Learning rate scheduler
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer=self.optimizer, 
            factor=1.0/np.cbrt(2), 
            patience=100, 
            verbose=True
        )
        
        # KL divergence controller
        if self.config.adaptive_kl:
            self.kl_controller = AdaptiveKLController(
                self.config.init_kl_coef,
                self.config.kl_target,
                self.config.horizon
            )
        else:
            self.kl_controller = FixedKLController(self.config.init_kl_coef)
        
        # Training statistics
        self.training_stats = {}
        self.step_count = 0
        
        # Set logger
        self.logger = logging.getLogger(__name__)

    def compute_gae_advantages(
        self, 
        rewards: torch.Tensor, 
        values: torch.Tensor,
        masks: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Use Generalized Advantage Estimation (GAE) to compute advantage function
        
        Args:
            rewards: Reward tensor [batch_size, seq_len]
            values: Value function output [batch_size, seq_len]
            masks: Valid position mask [batch_size, seq_len]
            
        Returns:
            advantages: Advantage function values
            returns: Return values
        """
        batch_size, seq_len = rewards.shape
        advantages = torch.zeros_like(rewards)
        last_gae_lam = torch.zeros(batch_size, device=rewards.device)
        
        for t in reversed(range(seq_len)):
            if t == seq_len - 1:
                next_non_terminal = torch.zeros(batch_size, device=rewards.device)
                next_values = torch.zeros(batch_size, device=rewards.device)
            else:
                next_non_terminal = masks[:, t + 1]
                next_values = values[:, t + 1]
            mask_t = masks[:, t]
            delta = (rewards[:, t] + 
                    self.config.gamma * next_values * next_non_terminal - 
                    values[:, t]) * mask_t
            
            advantages[:, t] = last_gae_lam = (
                delta + 
                self.config.gamma * self.config.lam * next_non_terminal * last_gae_lam
            ) * mask_t
        
        returns = advantages + values
        return advantages, returns

    def compute_policy_loss(
        self, 
        log_probs: torch.Tensor,
        old_log_probs: torch.Tensor,
        advantages: torch.Tensor,
        masks: torch.Tensor
    ) -> Tuple[torch.Tensor, Dict]:
        """
        Compute PPO policy loss
        
        Args:
            log_probs: New policy log probabilities
            old_log_probs: Old policy log probabilities  
            advantages: Advantage function values
            masks: Valid position mask
            
        Returns:
            loss: Policy loss
            stats: Training statistics
        """
        # Calculate importance sampling ratio
        ratio = torch.exp(log_probs - old_log_probs)
        
        # PPO clipped objective function
        surr1 = ratio * advantages
        surr2 = torch.clamp(
            ratio, 
            1.0 - self.config.cliprange, 
            1.0 + self.config.cliprange
        ) * advantages
        
        # Only calculate loss at valid positions
        policy_loss = -torch.where(
            advantages >= 0,
            torch.min(surr1, surr2),
            torch.max(surr1, surr2)
        )
        
        # Apply mask and calculate average loss
        weighted_policy_loss = (policy_loss * masks).sum(1) / masks.sum(1).clamp(min=1)
        policy_loss = weighted_policy_loss.mean()
        
        # Calculate clipping ratio
        clip_fraction = ((ratio > 1 + self.config.cliprange) | 
                        (ratio < 1 - self.config.cliprange)).float()
        clip_fraction = (clip_fraction * masks).sum() / masks.sum()
        
        stats = {
            'policy_loss': policy_loss.detach(),
            'clip_fraction': clip_fraction.detach(),
            'approx_kl': ((0.5 * (log_probs - old_log_probs).pow(2) * masks).sum() / masks.sum()).detach(),
            'ratio_mean': ((ratio * masks).sum() / masks.sum()).detach()
        }
        
        return policy_loss, stats

    def compute_value_loss(
        self, 
        values: torch.Tensor,
        old_values: torch.Tensor,
        returns: torch.Tensor,
        masks: torch.Tensor
    ) -> Tuple[torch.Tensor, Dict]:
        """
        Compute value function loss
        
        Args:
            values: New value function output
            old_values: Old value function output
            returns: Target return values
            masks: Valid position mask
            
        Returns:
            loss: Value function loss
            stats: Training statistics
        """
        # Value function clipping
        values_clipped = torch.clamp(
            values,
            old_values - self.config.cliprange_value,
            old_values + self.config.cliprange_value
        )
        
        # Calculate two losses
        vf_loss1 = (values - returns).pow(2)
        vf_loss2 = (values_clipped - returns).pow(2)
        vf_loss_tok = 0.5 * torch.max(vf_loss1, vf_loss2)   # æŒ‰token

        # åªåœ¨æœ‰æ•ˆä½ç½®è®¡ç®—lossï¼ˆmaskä¸º1çš„ä½ç½®ï¼‰
        vf_loss_tok = vf_loss_tok * masks

        # å½’ä¸€åŒ–ï¼šæ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆtokenæ•°
        vf_loss = vf_loss_tok.sum(1) / masks.sum(1).clamp(min=1)
        
        # Apply mask and calculate average loss
        vf_loss = vf_loss.mean()
        
        # Calculate clipping ratio
        vf_clip_fraction = (values != values_clipped).float()
        vf_clip_fraction = (vf_clip_fraction * masks).sum() / masks.sum()
        
        stats = {
            'value_loss': vf_loss.detach(),
            'value_clip_fraction': vf_clip_fraction.detach(),
            'values_mean': ((values * masks).sum() / masks.sum()).detach(),
            'returns_mean': ((returns * masks).sum() / masks.sum()).detach(),
            'returns_var': torch.var(returns[masks.bool()]).detach()
        }
        
        return vf_loss, stats

    def forward_pass(
        self, 
        input_ids: torch.Tensor, 
        attention_mask: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Model forward pass
        
        Args:
            input_ids: Input token sequence
            attention_mask: Attention mask
            
        Returns:
            logits: Output logits
            log_probs: Log probabilities
            values: Value function output
        """
        with torch.amp.autocast('cuda', dtype=torch.bfloat16):
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=None
            )
            
        logits, _, values = outputs
        
        # è®¡ç®—å¯¹æ•°æ¦‚ç‡ (å¯¹äºç”Ÿæˆçš„token)
        log_probs = logprobs_from_logits(logits[:, :-1], input_ids[:, 1:])
        
        return logits, log_probs, values[:, :-1]

    def compute_rewards_with_kl_penalty(
        self, 
        log_probs: torch.Tensor,
        ref_log_probs: torch.Tensor,
        reward_scores: torch.Tensor,
        mask: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, float]:
        """
        è®¡ç®—å¸¦KLæƒ©ç½šçš„æœ€ç»ˆå¥–åŠ±ï¼ˆæ ‡å‡†tokençº§åˆ«KLæƒ©ç½šï¼Œä¸åšåºåˆ—å¹³å‡ï¼‰

        Args:
            log_probs: å½“å‰ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
            ref_log_probs: å‚è€ƒç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
            reward_scores: å¤–éƒ¨å¥–åŠ±ï¼ˆå¦‚ç¼–è¯‘+AST+DFGï¼‰
            mask: å“åº”åºåˆ—çš„maskï¼ˆ1ä¸ºæœ‰æ•ˆï¼Œ0ä¸ºpaddingï¼‰
        Returns:
            final_rewards: æœ€ç»ˆå¥–åŠ±
            kl_penalties: KLæƒ©ç½š
            kl_coef: å½“å‰KLç³»æ•°
        """
        # é€tokenè®¡ç®—KLæ•£åº¦
        kl_divergence = log_probs - ref_log_probs  # [batch, seq_len]
        kl_penalties = -self.kl_controller.value * kl_divergence  # [batch, seq_len]
        kl_penalties = kl_penalties * mask  # åªå¯¹æœ‰æ•ˆtokenç”Ÿæ•ˆ

        # æœ€ç»ˆå¥–åŠ± = å¤–éƒ¨å¥–åŠ± + KLæƒ©ç½šï¼ˆé€tokenç›¸åŠ ï¼‰
        final_rewards = reward_scores + kl_penalties

        return final_rewards, kl_penalties, self.kl_controller.value

    def train_step(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        rewards: torch.Tensor,
        old_log_probs: torch.Tensor,
        old_values: torch.Tensor,
        response_mask: torch.Tensor
    ) -> Dict:
        """
        Execute one PPO training step
        
        Args:
            input_ids: Input sequence
            attention_mask: Attention mask
            rewards: Reward signal
            old_log_probs: Old policy log probabilities
            old_values: Old value function output
            response_mask: Mask for response sequences (1 for valid, 0 for padding)
            
        Returns:
            Training statistics dictionary
        """
        batch_size = input_ids.size(0)
        all_stats = []
        
        # Calculate GAE advantage
        # Note: rewards, old_log_probs, old_values now only contain response part
        advantages, returns = self.compute_gae_advantages(rewards, old_values, response_mask)
        
        # Standardize advantage
        normalized_advantages_stats = {}  # å­˜å‚¨æ ‡å‡†åŒ–åçš„ç»Ÿè®¡ä¿¡æ¯
        if response_mask.sum() > 1:
            # Standardize advantage at valid positions
            valid_advantages = advantages[response_mask.bool()]
            if len(valid_advantages) > 1:
                whitened_valid = whiten(valid_advantages)
                advantages = advantages.clone()
                advantages[response_mask.bool()] = whitened_valid
                
                # ğŸ”§ è®¡ç®—æ ‡å‡†åŒ–åadvantagesçš„ç»Ÿè®¡ä¿¡æ¯
                valid_normalized_advantages = advantages[response_mask.bool()]
                normalized_advantages_stats = {
                    'normalized_advantages_mean': (advantages * response_mask).sum() / response_mask.sum(),
                    'normalized_advantages_std': valid_normalized_advantages.std().item() if len(valid_normalized_advantages) > 1 else 0.0,
                    'normalized_advantages_max': valid_normalized_advantages.max().item() if len(valid_normalized_advantages) > 0 else 0.0,
                    'normalized_advantages_min': valid_normalized_advantages.min().item() if len(valid_normalized_advantages) > 0 else 0.0,
                }
            else:
                # æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä½¿ç”¨åŸå§‹æ•°æ®
                normalized_advantages_stats = {
                    'normalized_advantages_mean': (advantages * response_mask).sum() / response_mask.sum(),
                    'normalized_advantages_std': 0.0,
                    'normalized_advantages_max': advantages.max().item() if response_mask.sum() > 0 else 0.0,
                    'normalized_advantages_min': advantages.min().item() if response_mask.sum() > 0 else 0.0,
                }
        else:
            # æ²¡æœ‰æœ‰æ•ˆæ•°æ®
            normalized_advantages_stats = {
                'normalized_advantages_mean': 0.0,
                'normalized_advantages_std': 0.0,
                'normalized_advantages_max': 0.0,
                'normalized_advantages_min': 0.0,
            }
        
        # PPO multiple optimization epochs
        for epoch in range(self.config.ppo_epochs):
            # Randomly shuffle data
            perm = torch.randperm(batch_size, device=input_ids.device)
            
            # Mini-batch training with gradient accumulation
            accumulation_step = 0
            self.optimizer.zero_grad()  # åœ¨æ¯ä¸ªepochå¼€å§‹æ—¶æ¸…é›¶æ¢¯åº¦
            
            # ğŸ”§ Fix: æ¯ä¸ªPPO epoché‡æ–°åˆå§‹åŒ–ç»Ÿè®¡åˆ—è¡¨ï¼Œé¿å…ç´¯ç§¯
            epoch_stats = []
            
            for start in range(0, batch_size, self.config.minibatch_size):
                end = min(start + self.config.minibatch_size, batch_size)
                idx = perm[start:end]
                
                # Get mini-batch data
                mb_input_ids = input_ids[idx]
                mb_attention_mask = attention_mask[idx]
                mb_rewards = rewards[idx]
                mb_old_log_probs = old_log_probs[idx]
                mb_old_values = old_values[idx]
                mb_advantages = advantages[idx]
                mb_returns = returns[idx]
                mb_response_mask = response_mask[idx]
                
                # ğŸ”§ New: æ ¹æ®è®­ç»ƒé˜¶æ®µå†³å®šæ˜¯å¦éœ€è¦è®¡ç®—policyï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
                if self.step_count < self.config.critic_warmup_steps:
                    # Criticé¢„çƒ­é˜¶æ®µï¼šåªéœ€è¦è®¡ç®—valueï¼Œè·³è¿‡policyè®¡ç®—ä»¥èŠ‚çœæ˜¾å­˜
                    # åªè¿›è¡Œvalueç›¸å…³çš„å‰å‘ä¼ æ’­
                    with torch.amp.autocast('cuda', dtype=torch.bfloat16):
                        outputs = self.model(
                            input_ids=mb_input_ids,
                            attention_mask=mb_attention_mask,
                            labels=None
                        )
                    
                    _, _, full_values = outputs  # åªéœ€è¦values
                    # ğŸ”§ Fix: ä¿æŒä¸forward_passä¸€è‡´ï¼Œå»æ‰æœ€åä¸€ä½
                    full_values = full_values[:, :-1]  # ä¸forward_passä¿æŒä¸€è‡´
                    source_len = mb_input_ids.size(1) - mb_rewards.size(1)
                    response_values = full_values[:, source_len-1:]
                    
                    # åªè®¡ç®—value loss
                    value_loss, value_stats = self.compute_value_loss(
                        response_values, mb_old_values, mb_returns, mb_response_mask
                    )
                    
                    # åˆ›å»ºç©ºçš„policy_statsä»¥ä¿æŒç»Ÿè®¡ä¿¡æ¯ä¸€è‡´æ€§
                    policy_stats = {
                        'policy_loss': torch.tensor(0.0),
                        'entropy': torch.tensor(0.0),
                        'approx_kl': torch.tensor(0.0),
                        'clipfrac': torch.tensor(0.0),
                        'ratio_mean': torch.tensor(1.0)
                    }
                    
                    total_loss = self.config.vf_coef * value_loss
                    
                    # è®°å½•å½“å‰å¤„äºé¢„çƒ­é˜¶æ®µ
                    if accumulation_step == 1:  # åªåœ¨æ¯ä¸ªmini-batchçš„ç¬¬ä¸€æ­¥è®°å½•ä¸€æ¬¡
                        self.logger.debug(f"Critic warmup phase: step {self.step_count}/{self.config.critic_warmup_steps}")
                        
                else:
                    # æ­£å¸¸è®­ç»ƒé˜¶æ®µï¼šéœ€è¦å®Œæ•´çš„å‰å‘ä¼ æ’­
                    _, full_log_probs, full_values = self.forward_pass(mb_input_ids, mb_attention_mask)
                    
                    # Extract response part for loss calculation
                    # Need to calculate source length to determine response start position
                    source_len = mb_input_ids.size(1) - mb_rewards.size(1)
                    response_log_probs = full_log_probs[:, source_len-1:]
                    response_values = full_values[:, source_len-1:]
                    
                    # Calculate loss (only for response part)
                    policy_loss, policy_stats = self.compute_policy_loss(
                        response_log_probs, mb_old_log_probs, mb_advantages, mb_response_mask
                    )
                    
                    value_loss, value_stats = self.compute_value_loss(
                        response_values, mb_old_values, mb_returns, mb_response_mask
                    )
                    
                    # æ­£å¸¸è®­ç»ƒé˜¶æ®µï¼šåŒæ—¶è®­ç»ƒactorå’Œcritic
                    total_loss = policy_loss + self.config.vf_coef * value_loss
                
                # æ¢¯åº¦ç´¯ç§¯ï¼šå°†lossé™¤ä»¥ç´¯ç§¯æ­¥æ•°
                scaled_loss = total_loss / self.config.gradient_accumulation_steps
                
                # Backward propagation
                scaled_loss.backward()
                
                # ç´¯ç§¯è®¡æ•°å™¨
                accumulation_step += 1
                
                # å½“ç´¯ç§¯æ­¥æ•°è¾¾åˆ°è®¾å®šå€¼æ—¶ï¼Œè¿›è¡Œå‚æ•°æ›´æ–°
                if accumulation_step % self.config.gradient_accumulation_steps == 0:
                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                    
                    self.optimizer.step()
                    self.optimizer.zero_grad()
                
                # Record statistics (ä½¿ç”¨åŸå§‹çš„total_lossè€Œä¸æ˜¯scaled_loss)
                # ğŸ”§ Fix: è½¬æ¢ä¸ºæ ‡é‡ä»¥é¿å…æ˜¾å­˜ç´¯ç§¯
                stats = {
                    'total_loss': total_loss.detach().cpu().item(),  # è½¬ä¸ºCPUæ ‡é‡
                    **{k: v.detach().cpu().item() if torch.is_tensor(v) else v for k, v in policy_stats.items()},
                    **{k: v.detach().cpu().item() if torch.is_tensor(v) else v for k, v in value_stats.items()}
                }
                epoch_stats.append(stats)
            
            # å¤„ç†æœ€åå¯èƒ½å‰©ä½™çš„æ¢¯åº¦ï¼ˆå¦‚æœbatchä¸èƒ½è¢«gradient_accumulation_stepsæ•´é™¤ï¼‰
            if accumulation_step % self.config.gradient_accumulation_steps != 0:
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                
                self.optimizer.step()
                self.optimizer.zero_grad()
            
            # ğŸ”§ Fix: å°†å½“å‰epochçš„ç»Ÿè®¡ä¿¡æ¯æ·»åŠ åˆ°æ€»åˆ—è¡¨
            all_stats.extend(epoch_stats)
        
        # Aggregate statistics
        # ğŸ”§ Fix: å¤„ç†æ ‡é‡ç»Ÿè®¡ä¿¡æ¯
        final_stats = {}
        for key in all_stats[0].keys():
            # è®¡ç®—æ‰€æœ‰mini-batchçš„å¹³å‡å€¼
            values = [s[key] for s in all_stats]
            final_stats[key] = torch.tensor(sum(values) / len(values))
        
        # ğŸ”§ æ·»åŠ æ ‡å‡†åŒ–åadvantagesçš„ç»Ÿè®¡ä¿¡æ¯
        final_stats.update(normalized_advantages_stats)
        
        return final_stats

    def step(
        self,
        source_ids: torch.Tensor,
        source_mask: torch.Tensor, 
        response_ids: torch.Tensor,
        response_ids_ref: torch.Tensor,
        reward_scores: torch.Tensor,
        response_mask: torch.Tensor
    ) -> Dict:
        """
        PPOCoder single step training main function
        
        Args:
            source_ids: Source code sequence
            source_mask: Source code mask
            response_ids: Generated response sequence
            response_ids_ref: Reference response sequence
            reward_scores: External reward scores (compiler + AST + DFG)
            response_mask: Mask for response sequences (1 for valid, 0 for padding)
            
        Returns:
            Training statistics dictionary
        """
        timing = {}
        t0 = time.time()
        
        # 1. Build complete input sequence
        full_input_ids = torch.cat([source_ids, response_ids], dim=1)
        # Use the provided response_mask instead of creating all-ones mask
        full_attention_mask = torch.cat([source_mask, response_mask], dim=1)
        
        # 2. Get current policy output (æ¡ä»¶æ€§è®¡ç®—ï¼Œåœ¨Criticé¢„çƒ­é˜¶æ®µè·³è¿‡)
        timing['forward_start'] = time.time()
        
        if self.step_count < self.config.critic_warmup_steps:
            # ğŸ”§ Criticé¢„çƒ­é˜¶æ®µï¼šè·³è¿‡æ˜‚è´µçš„log_probsè®¡ç®—ï¼Œåªè®¡ç®—values
            with torch.no_grad():
                with torch.amp.autocast('cuda', dtype=torch.bfloat16):
                    outputs = self.model(
                        input_ids=full_input_ids,
                        attention_mask=full_attention_mask,
                        labels=None
                    )
                _, _, old_values = outputs
                old_values = old_values[:, :-1]  # ä¸forward_passä¿æŒä¸€è‡´
                
                # åˆ›å»ºç©ºçš„log_probsä»¥ä¿æŒæ¥å£ä¸€è‡´
                old_log_probs = torch.zeros(
                    old_values.shape, device=old_values.device, dtype=old_values.dtype
                )
                ref_log_probs = torch.zeros_like(old_log_probs)
        else:
            # æ­£å¸¸è®­ç»ƒé˜¶æ®µï¼šå®Œæ•´è®¡ç®—
            with torch.no_grad():
                _, old_log_probs, old_values = self.forward_pass(
                    full_input_ids, full_attention_mask
                )
                
                # Get reference model output
                with torch.amp.autocast('cuda', dtype=torch.bfloat16):
                    ref_outputs = self.ref_model(
                        input_ids=full_input_ids,
                        attention_mask=full_attention_mask,
                        labels=None
                    )
                ref_logits = ref_outputs[0]
                ref_log_probs = logprobs_from_logits(
                    ref_logits[:, :-1], full_input_ids[:, 1:]
                )
        
        timing['forward_end'] = time.time()
        
        # 3. Calculate reward (including KL penalty)
        timing['reward_start'] = time.time()
        
        # Only calculate reward for response part
        source_len = source_ids.size(1)
        response_log_probs = old_log_probs[:, source_len-1:]
        response_ref_log_probs = ref_log_probs[:, source_len-1:]
        response_values = old_values[:, source_len-1:]

        # åˆ›å»ºå’Œresponse_log_probsç›¸åŒå¤§å°çš„å…¨0å¥–åŠ±æ•°ç»„
        rewards = torch.zeros_like(response_log_probs)
        
        # è·å–batchä¸­æ¯ä¸ªåºåˆ—çš„EOSä½ç½®
        eos_positions = []
        for seq in response_ids:
            # æ‰¾åˆ°EOS tokençš„ä½ç½®
            eos_pos = (seq == self.config.tokenizer.eos_token_id).nonzero()
            if len(eos_pos) > 0:
                eos_positions.append(eos_pos[0].item())
            else:
                eos_positions.append(len(seq) - 1)
                
        # å°†å¥–åŠ±å€¼æ”¾åˆ°å¯¹åº”çš„EOSä½ç½®
        for i, (eos_pos, reward) in enumerate(zip(eos_positions, reward_scores)):
            # reward_scoresæ˜¯ä¸€ä¸ªåŒ…å«æ¯ä¸ªåºåˆ—æ€»å¥–åŠ±å€¼çš„åˆ—è¡¨ï¼Œæ¯ä¸ªåºåˆ—åªæœ‰ä¸€ä¸ªé0å€¼
            # æå–rewardå¼ é‡ä¸­çš„å®é™…å¥–åŠ±å€¼
            if isinstance(reward, torch.Tensor):
                # å¦‚æœæ˜¯å¼ é‡ï¼Œæ‰¾åˆ°éé›¶å…ƒç´ 
                non_zero_indices = (reward != 0).nonzero(as_tuple=True)[0]
                if len(non_zero_indices) > 0:
                    reward_value = reward[non_zero_indices[0]].item()
                else:
                    reward_value = reward.sum().item()
            else:
                reward_value = float(reward)
            
            # å°†è¯¥å€¼æ”¾åˆ°å¯¹åº”åºåˆ—çš„EOSä½ç½®
            rewards[i, eos_pos] = reward_value

        # ğŸ”§ Criticé¢„çƒ­é˜¶æ®µè·³è¿‡KL penaltyè®¡ç®—
        if self.step_count < self.config.critic_warmup_steps:
            # Criticé¢„çƒ­é˜¶æ®µï¼šä¸è®¡ç®—KL penaltyï¼Œç›´æ¥ä½¿ç”¨åŸå§‹rewards
            final_rewards = rewards
            kl_penalties = torch.zeros_like(rewards)
            kl_coef = self.config.init_kl_coef
        else:
            # æ­£å¸¸è®­ç»ƒé˜¶æ®µï¼šè®¡ç®—KL penalty
            final_rewards, kl_penalties, kl_coef = self.compute_rewards_with_kl_penalty(
                response_log_probs, response_ref_log_probs, rewards, response_mask
            )
        
        timing['reward_end'] = time.time()
        
        # 4. PPO training step
        timing['train_start'] = time.time()
        
        # Calculate GAE advantages for statistics (before training)
        advantages, returns = self.compute_gae_advantages(final_rewards, response_values, response_mask)
        
        # Ensure tensors passed to train_step only contain response part, same shape
        train_stats = self.train_step(
            full_input_ids,
            full_attention_mask, 
            final_rewards,           # [batch_size, response_len]
            response_log_probs,      # [batch_size, response_len] 
            response_values,         # [batch_size, response_len]
            response_mask            # [batch_size, response_len]
        )
        
        timing['train_end'] = time.time()
        
        # 5. Update KL controller
        mean_kl = ((response_log_probs - response_ref_log_probs) * response_mask).sum() / response_mask.sum()
        self.kl_controller.update(mean_kl.item(), self.config.batch_size)
        
        # 6. Update learning rate scheduler
        self.scheduler.step(train_stats['total_loss'])
        
        # 7. Integrate statistics
        # Calculate advantages statistics for monitoring
        valid_advantages = advantages[response_mask.bool()]  # åŸå§‹advantages (æ ‡å‡†åŒ–å‰)
        
        stats = {
            'ppo/mean_kl': mean_kl.item(),
            'ppo/kl_coef': kl_coef,
            'ppo/mean_reward': (rewards * response_mask).sum() / response_mask.sum(),
            'ppo/mean_kl_penalty': (kl_penalties * response_mask).sum() / response_mask.sum(),
            'ppo/mean_final_reward': (final_rewards * response_mask).sum() / response_mask.sum(),
            
            # åŸå§‹advantagesç»Ÿè®¡ (æ ‡å‡†åŒ–å‰)
            'ppo/advantages_raw_mean': (advantages * response_mask).sum() / response_mask.sum(),
            'ppo/advantages_raw_std': valid_advantages.std().item() if len(valid_advantages) > 1 else 0.0,
            'ppo/advantages_raw_max': valid_advantages.max().item() if len(valid_advantages) > 0 else 0.0,
            'ppo/advantages_raw_min': valid_advantages.min().item() if len(valid_advantages) > 0 else 0.0,
            
            # æ ‡å‡†åŒ–åadvantagesç»Ÿè®¡ (ä»train_stepè¿”å›)
            'ppo/advantages_normalized_mean': train_stats.get('normalized_advantages_mean', 0.0),
            'ppo/advantages_normalized_std': train_stats.get('normalized_advantages_std', 0.0),
            'ppo/advantages_normalized_max': train_stats.get('normalized_advantages_max', 0.0),
            'ppo/advantages_normalized_min': train_stats.get('normalized_advantages_min', 0.0),
            
            # å‘åå…¼å®¹ï¼šä¿æŒåŸæœ‰çš„advantages_meané”®
            'ppo/advantages_mean': (advantages * response_mask).sum() / response_mask.sum(),
            
            'ppo/returns_mean': (returns * response_mask).sum() / response_mask.sum(),
            **{f'ppo/{k}': v.item() if hasattr(v, 'item') else v 
               for k, v in train_stats.items()},
            **{f'time/ppo/{k}': v for k, v in timing.items()}
        }
        
        # Backward compatibility: add old key name mapping (convert to tensor to support .item() call)
        def to_tensor_compat(value):
            """Convert value to compatible tensor, support .item() method call"""
            if isinstance(value, (int, float)):
                return torch.tensor(float(value))
            return value
        
        backward_compat_stats = {
            # Old objective/ key names
            'objective/kl': to_tensor_compat(stats['ppo/mean_kl']),
            'objective/kl_coef': to_tensor_compat(stats['ppo/kl_coef']),
            'objective/entropy': to_tensor_compat(stats.get('ppo/policy_loss', 0.0)),
            
            # Old reward key names
            'ppo/mean_non_score_reward': to_tensor_compat(stats['ppo/mean_kl_penalty']),
            'ppo/mean_score_reward': to_tensor_compat(stats['ppo/mean_reward']),
            
            # Old loss key name format (critical fix: these need to support .item() call)
            'ppo/loss/total': to_tensor_compat(stats.get('ppo/total_loss', 0.0)),
            'ppo/loss/policy': to_tensor_compat(stats.get('ppo/policy_loss', 0.0)),
            'ppo/loss/value': to_tensor_compat(stats.get('ppo/value_loss', 0.0)),
            
            # Old policy statistics key names - ä¿®æ­£é€»è¾‘
            'ppo/policy/advantages_mean': to_tensor_compat(stats['ppo/advantages_mean']),
            'ppo/returns/mean': to_tensor_compat(stats['ppo/returns_mean']),
            'ppo/val/mean': to_tensor_compat(stats.get('ppo/values_mean', 0.0)),
            'ppo/ratio_mean': to_tensor_compat(stats.get('ppo/ratio_mean', 0.0)),
        }
        
        # Merge backward compatible statistics
        stats.update(backward_compat_stats)
        
        # Calculate explained variance
        if 'returns_var' in train_stats and train_stats['returns_var'] > 0:
            explained_var = 1 - train_stats['value_loss'] / train_stats['returns_var']
            explained_var_value = explained_var.item() if hasattr(explained_var, 'item') else explained_var
            stats['ppo/explained_variance'] = explained_var_value
            # Use inline conversion to ensure backward compatibility
            stats['ppo/val/var_explained'] = torch.tensor(float(explained_var_value))
        
        timing['total'] = time.time() - t0
        stats['time/ppo/total'] = timing['total']
        
        # Clean up GPU cache
        torch.cuda.empty_cache()
        
        # ğŸ”§ New: æ£€æŸ¥æ˜¯å¦åˆšå¥½å®Œæˆcriticé¢„çƒ­é˜¶æ®µ
        if (self.step_count == self.config.critic_warmup_steps - 1 and 
            self.config.critic_warmup_steps > 0):
            self.logger.info(f"ğŸ¯ Critic warmup completed! Starting joint actor-critic training from step {self.step_count + 1}")
        
        self.step_count += 1
        return stats

    def save_checkpoint(self, path: str) -> None:
        """Save training checkpoint"""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'kl_controller_value': self.kl_controller.value,
            'step_count': self.step_count,
            'config': self.config
        }
        torch.save(checkpoint, path)
        self.logger.info(f"Checkpoint saved to: {path}")

    def load_checkpoint(self, path: str) -> None:
        """Load training checkpoint"""
        checkpoint = torch.load(path, map_location=self.config.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.kl_controller.value = checkpoint['kl_controller_value']
        self.step_count = checkpoint['step_count']
        self.logger.info(f"Checkpoint loaded from {path}")


# Backward compatible alias
PPOTrainer = PPOCoderTrainer


def create_ppocoder_trainer(
    model, 
    ref_model, 
    learning_rate: float = 1e-5,
    init_kl_coef: float = 0.1,
    kl_target: float = 1.0,
    **kwargs
) -> PPOCoderTrainer:
    """
    Create PPOCoder trainer convenience function
    
    Args:
        model: Training model
        ref_model: Reference model
        learning_rate: Learning rate
        init_kl_coef: Initial KL coefficient
        kl_target: KL target value
        **kwargs: Other configuration parameters
        
    Returns:
        PPOCoderTrainer instance
    """
    config = PPOConfig(
        lr=learning_rate,
        init_kl_coef=init_kl_coef,
        kl_target=kl_target,
        **kwargs
    )
    
    return PPOCoderTrainer(model, ref_model, config)
