# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02-ppo.ipynb (unless otherwise specified).

__all__ = ['AdaptiveKLController', 'FixedKLController', 'PPOCoderTrainer']

# Cell
import numpy as np
import torch
import torch.optim as optim
import time
import logging
from torch.optim import AdamW
from transformers import RobertaTokenizer
from utils import logprobs_from_logits
from dataclasses import dataclass
from typing import Dict, Tuple, Optional


@torch.no_grad()
def whiten_advantages(
    advantages: torch.Tensor,   # [B, T]
    mask: torch.Tensor,         # [B, T] in {0,1} or bool
    mode: str = "per_sequence", # "per_sequence"(default) or "per_batch"
    clip_value: float = None,   # optional: clip before whitening, e.g. 10.0
    eps: float = 1e-8,
    detach_output: bool = True  # PPO often uses A as weights, default no backprop
) -> torch.Tensor:
    """
    Single-GPU, streamlined and robust advantage normalization:
    - mask-aware
    - per-sequence(recommended) or per-batch
    - statistics in fp32; output preserves original dtype
    - optional gentle clipping; only one whitening
    """
    assert advantages.shape == mask.shape and advantages.dim() == 2
    A = advantages.clone()
    M = mask.to(dtype=torch.float32)
    A = A * (M.sum(dim=1, keepdim=True) > 0).float()  # Zero out samples with no valid tokens, defensive

    if clip_value is not None:
        A = A.clamp_(-clip_value, clip_value)

    A32 = A.float()

    if mode == "per_sequence":
        valid = M.sum(dim=1, keepdim=True).clamp_min(1.0)
        mean  = (A32 * M).sum(dim=1, keepdim=True) / valid
        var   = ((A32 - mean)**2 * M).sum(dim=1, keepdim=True) / valid
        std   = (var + eps).sqrt()
        A32   = (A32 - mean) / std
    elif mode == "per_batch":
        valid = M.sum().clamp_min(1.0)
        mean  = (A32 * M).sum() / valid
        var   = ((A32 - mean)**2 * M).sum() / valid
        std   = (var + eps).sqrt()
        A32   = (A32 - mean) / std
    else:
        raise ValueError("mode must be 'per_sequence' or 'per_batch'")

    A32 = A32 * M  # Only keep valid tokens
    Aout = A32.to(advantages.dtype)
    return Aout.detach() if detach_output else Aout


@dataclass
class PPOConfig:
    """PPOcoder configuration parameters"""
    # Learning parameters
    lr: float = 1e-5
    adam_eps: float = 1e-8
    
    # PPO parameters
    gamma: float = 1
    lam: float = 0.99
    cliprange: float = 0.2
    cliprange_value: float = 0.2
    vf_coef: float = 0.5
    ppo_epochs: int = 2
    
    # Batch processing parameters
    batch_size: int = 48
    minibatch_size: int = 12
    forward_batch_size: int = 16
    gradient_accumulation_steps: int = 1
    
    # KL control parameters
    init_kl_coef: float = 0.05
    kl_target: float = 1
    adaptive_kl: bool = True
    horizon: int = 10000
    
    # KL penalty distribution strategy
    kl_penalty_strategy: str = "eos_concentrated"  # "eos_concentrated" or "distributed"
    
    # KL penalty application mode
    kl_penalty_mode: str = "reward"  # "reward" or "loss"
    
    # Other configuration
    device: str = "cuda"
    tokenizer: RobertaTokenizer = None
    max_grad_norm: float = 1.0  # Gradient clipping threshold


class AdaptiveKLController:
    """Adaptive KL divergence controller"""
    def __init__(self, init_kl_coef: float, target: float, horizon: int = 10000):
        self.value, self.target, self.horizon = init_kl_coef, target, horizon

    def update(self, current_kl: float, n_steps: int) -> None:
        proportional_error = np.clip(current_kl / self.target - 1, -0.2, 0.2)
        self.value *= 1 + proportional_error * n_steps / self.horizon

class FixedKLController:
    """Fixed KL divergence controller"""
    def __init__(self, kl_coef: float):
        self.value = kl_coef
    def update(self, current_kl: float, n_steps: int) -> None:
        pass


class PPOCoderTrainer:
    """PPOCoder trainer for code generation with execution feedback"""

    def __init__(self, model, ref_model, config: Optional[PPOConfig] = None, **kwargs):
        self.config = config or PPOConfig()
        
        # Backward compatibility parameter mapping
        param_mapping = {'adap_kl_ctrl': 'adaptive_kl', 'target': 'kl_target'}
        for key, value in kwargs.items():
            mapped_key = param_mapping.get(key, key)
            if hasattr(self.config, mapped_key):
                setattr(self.config, mapped_key, value)
        
        self.model, self.ref_model = model, ref_model
        self.optimizer = AdamW(model.parameters(), lr=self.config.lr, eps=self.config.adam_eps)
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, factor=1.0/np.cbrt(2), patience=100, verbose=True)
        
        # KL controller
        self.kl_controller = (AdaptiveKLController(self.config.init_kl_coef, self.config.kl_target, self.config.horizon) 
                             if self.config.adaptive_kl else FixedKLController(self.config.init_kl_coef))
        
        self.training_stats, self.step_count = {}, 0
        self.logger = logging.getLogger(__name__)

    def compute_gae_advantages(self, rewards: torch.Tensor, values: torch.Tensor, masks: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Compute advantage function using GAE"""
        calc_dtype = rewards.dtype
        values, masks = values.to(calc_dtype), masks.to(calc_dtype)
        batch_size, seq_len = rewards.shape
        advantages = torch.zeros_like(rewards, dtype=calc_dtype)
        last_gae_lam = torch.zeros(batch_size, device=rewards.device, dtype=calc_dtype)
        
        # ðŸ”§ Important: use detached values to compute advantages, prevent gradient propagation
        with torch.no_grad():
            values_detached = values.detach()
            for t in reversed(range(seq_len)):
                next_non_terminal = masks[:, t + 1] if t < seq_len - 1 else torch.zeros(batch_size, device=rewards.device)
                next_values = values_detached[:, t + 1] if t < seq_len - 1 else torch.zeros(batch_size, device=rewards.device)
                mask_t = masks[:, t]
                delta = (rewards[:, t] + self.config.gamma * next_values * next_non_terminal - values_detached[:, t]) * mask_t
                advantages[:, t] = last_gae_lam = (delta + self.config.gamma * self.config.lam * next_non_terminal * last_gae_lam) * mask_t
        
        # returns still need original values to compute value loss
        returns = advantages.detach() + values
        return advantages.detach(), returns

    def compute_policy_loss(self, log_probs: torch.Tensor, old_log_probs: torch.Tensor, advantages: torch.Tensor, masks: torch.Tensor) -> Tuple[torch.Tensor, Dict]:
        """Compute PPO policy loss - fix length bias"""
        ratio = torch.exp(log_probs - old_log_probs)
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1.0 - self.config.cliprange, 1.0 + self.config.cliprange) * advantages
        
        policy_loss = -torch.where(advantages >= 0, torch.min(surr1, surr2), torch.max(surr1, surr2))
        
        # ðŸ”§ Fix length bias: divide by valid token count per sample first, then batch mean
        per_sample_loss = (policy_loss * masks).sum(dim=1) / masks.sum(dim=1).clamp(min=1)
        policy_loss = per_sample_loss.mean()
        
        clip_fraction = ((ratio > 1 + self.config.cliprange) | (ratio < 1 - self.config.cliprange)).float()
        clip_fraction = (clip_fraction * masks).sum() / masks.sum()
        
        return policy_loss, {
            'policy_loss': policy_loss.detach(),
            'clip_fraction': clip_fraction.detach(),
            'approx_kl': ((0.5 * (log_probs - old_log_probs).pow(2) * masks).sum() / masks.sum()).detach(),
            'ratio_mean': ((ratio * masks).sum() / masks.sum()).detach()
        }

    def compute_value_loss(self, values: torch.Tensor, old_values: torch.Tensor, returns: torch.Tensor, masks: torch.Tensor) -> Tuple[torch.Tensor, Dict]:
        """Compute value function loss"""
        values_clipped = torch.clamp(values, old_values - self.config.cliprange_value, old_values + self.config.cliprange_value)
        vf_loss1, vf_loss2 = (values - returns).pow(2), (values_clipped - returns).pow(2)
        vf_loss = (0.5 * torch.max(vf_loss1, vf_loss2) * masks).sum(1) / masks.sum(1).clamp(min=1)
        vf_loss = vf_loss.mean()
        
        vf_clip_fraction = ((values != values_clipped).float() * masks).sum() / masks.sum()
        
        return vf_loss, {
            'value_loss': vf_loss.detach(),
            'value_clip_fraction': vf_clip_fraction.detach(),
            'values_mean': ((values * masks).sum() / masks.sum()).detach(),
            'returns_mean': ((returns * masks).sum() / masks.sum()).detach(),
            'returns_var': torch.var(returns[masks.bool()]).detach()
        }

    def forward_pass(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Model forward pass"""
        with torch.amp.autocast('cuda', dtype=torch.bfloat16):
            logits, _, values = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=None)
        log_probs = logprobs_from_logits(logits[:, :-1], input_ids[:, 1:])
        return logits, log_probs, values[:, :-1]

    def compute_rewards_with_kl_penalty(self, log_probs: torch.Tensor, ref_log_probs: torch.Tensor, reward_scores: torch.Tensor, mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, float]:
        """Compute final reward with KL penalty - supports two strategies"""
        if self.config.kl_penalty_strategy == "eos_concentrated":
            # Strategy 1: KL penalty concentrated at EOS position, considering sequence length normalization
            batch_size, seq_len = log_probs.shape
            
            # Calculate average KL divergence per sequence (normalized per token)
            kl_per_sequence = torch.zeros(batch_size, device=log_probs.device)
            for i in range(batch_size):
                # Find valid length of this sequence
                valid_length = mask[i].sum().item()
                if valid_length > 0:
                    # Calculate KL divergence for this sequence and normalize
                    valid_length_int = int(valid_length)  # Ensure integer
                    seq_kl_sum = (log_probs[i, :valid_length_int] - ref_log_probs[i, :valid_length_int]).sum()
                    # ðŸ”§ Use average KL divergence to avoid sequence length bias
                    kl_per_sequence[i] = seq_kl_sum / valid_length
            
            # Create KL penalty tensor, only place normalized KL penalty at EOS position
            kl_penalties = torch.zeros_like(reward_scores)
            
            # Find EOS position for each sequence
            for i in range(batch_size):
                # Find EOS position (last valid token)
                valid_positions = (mask[i] > 0.5).nonzero(as_tuple=True)[0]
                if len(valid_positions) > 0:
                    eos_pos = valid_positions[-1].item()
                    # Place normalized KL penalty at EOS position
                    kl_penalties[i, eos_pos] = -self.kl_controller.value * kl_per_sequence[i]
        
        elif self.config.kl_penalty_strategy == "distributed":
            # Strategy 2: Original PPO approach - KL penalty distributed across all token positions
            kl_penalties = -self.kl_controller.value * (log_probs.detach() - ref_log_probs.detach()) * mask
        
        else:
            raise ValueError(f"Unsupported KL penalty strategy: {self.config.kl_penalty_strategy}")
        
        # Decide whether to apply KL penalty to reward based on mode
        if self.config.kl_penalty_mode == "reward":
            # Mode 1: KL penalty applied to reward (original PPO approach)
            final_rewards = reward_scores + kl_penalties
        elif self.config.kl_penalty_mode == "loss":
            # Mode 2: KL penalty not applied to reward, will be added during loss calculation
            final_rewards = reward_scores
        else:
            raise ValueError(f"Unsupported KL penalty mode: {self.config.kl_penalty_mode}")
        
        return final_rewards, kl_penalties, self.kl_controller.value

    def train_step(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, rewards: torch.Tensor, 
                   old_log_probs: torch.Tensor, old_values: torch.Tensor, response_mask: torch.Tensor) -> Dict:
        """Execute one PPO training step"""
        batch_size, all_stats = input_ids.size(0), []
        advantages, returns = self.compute_gae_advantages(rewards, old_values, response_mask)
        
        # EOS advantage statistics
        mask_counts = response_mask.sum(dim=1, dtype=torch.long)
        eos_indices = (mask_counts - 1).clamp(min=0)
        batch_indices = torch.arange(batch_size, device=advantages.device)
        adv_at_eos_before = advantages[batch_indices, eos_indices].mean().item()

        # Use whiten_advantages for normalization
        advantages = whiten_advantages(
            advantages=advantages,
            mask=response_mask,
            mode="per_sequence",  # Per-sequence normalization
            clip_value=10.0,      # Gentle clipping
            eps=1e-8,
            detach_output=True
        )
        
        # Statistics
        valid_mask = response_mask.bool()
        has_valid_data = valid_mask.any()
        
        if has_valid_data:
            valid_normalized_advantages = advantages[valid_mask]
            adv_at_eos_after = advantages[batch_indices, eos_indices].mean().item()
            compression_ratio = adv_at_eos_after / adv_at_eos_before if adv_at_eos_before != 0 else 0.0
            normalized_advantages_stats = {
                'normalized_advantages_mean': valid_normalized_advantages.mean().item(),
                'normalized_advantages_std': valid_normalized_advantages.std().item(),
                'debug/advantage_at_eos_before_whiten': adv_at_eos_before,
                'debug/advantage_at_eos_after_whiten': adv_at_eos_after,
                'debug/advantage_at_eos_compression_ratio': compression_ratio
            }
        else:
            normalized_advantages_stats = {
                'normalized_advantages_mean': 0.0, 'normalized_advantages_std': 0.0,
                'debug/advantage_at_eos_before_whiten': adv_at_eos_before,
                'debug/advantage_at_eos_after_whiten': adv_at_eos_before,
                'debug/advantage_at_eos_compression_ratio': 1.0 if adv_at_eos_before != 0 else 0.0
            }

        
        # PPO training loop
        for epoch in range(self.config.ppo_epochs):
            perm = torch.randperm(batch_size, device=input_ids.device)
            accumulation_step, epoch_stats = 0, []
            self.optimizer.zero_grad()
            
            for start in range(0, batch_size, self.config.minibatch_size):
                idx = perm[start:min(start + self.config.minibatch_size, batch_size)]
                # Get mini-batch data
                mb_data = [t[idx] for t in [input_ids, attention_mask, rewards, old_log_probs, old_values, advantages, returns, response_mask]]
                mb_input_ids, mb_attention_mask, mb_rewards, mb_old_log_probs, mb_old_values, mb_advantages, mb_returns, mb_response_mask = mb_data
                
                # Forward pass and loss calculation
                _, full_log_probs, full_values = self.forward_pass(mb_input_ids, mb_attention_mask)
                source_len = mb_input_ids.size(1) - mb_rewards.size(1)
                response_log_probs, response_values = full_log_probs[:, source_len-1:], full_values[:, source_len-1:]

                policy_loss, policy_stats = self.compute_policy_loss(response_log_probs, mb_old_log_probs, mb_advantages, mb_response_mask)
                value_loss, value_stats = self.compute_value_loss(response_values, mb_old_values, mb_returns, mb_response_mask)
                
                # Add KL penalty in loss mode
                kl_loss = 0.0
                if self.config.kl_penalty_mode == "loss":
                    # Calculate KL divergence loss
                    kl_diff = (response_log_probs - mb_old_log_probs) * mb_response_mask
                    kl_loss = self.kl_controller.value * kl_diff.sum(dim=1).mean()
                
                total_loss = policy_loss + self.config.vf_coef * value_loss + kl_loss
                
                # ðŸ”§ Fix gradient accumulation: scale loss to avoid effective learning rate amplification
                scaled_loss = total_loss / self.config.gradient_accumulation_steps
                scaled_loss.backward()
                accumulation_step += 1
                
                if accumulation_step % self.config.gradient_accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
                    self.optimizer.step()
                    self.optimizer.zero_grad()
                
                # Record statistics (use original loss, not scaled_loss)
                stats = {'total_loss': total_loss.detach().cpu().item()}
                if self.config.kl_penalty_mode == "loss":
                    stats['kl_loss'] = kl_loss.detach().cpu().item()
                stats.update({k: v.detach().cpu().item() if torch.is_tensor(v) else v for k, v in {**policy_stats, **value_stats}.items()})
                epoch_stats.append(stats)
            
            # Handle remaining gradients
            if accumulation_step % self.config.gradient_accumulation_steps != 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
                self.optimizer.step()
                self.optimizer.zero_grad()
            all_stats.extend(epoch_stats)
        
        # Aggregate statistics
        all_keys = set().union(*[s.keys() for s in all_stats])
        final_stats = {key: torch.tensor(sum(float(s[key]) for s in all_stats if key in s) / sum(1 for s in all_stats if key in s)) 
                      for key in all_keys if any(key in s for s in all_stats)}
        final_stats.update(normalized_advantages_stats)
        return final_stats

    def step(
        self,
        source_ids: torch.Tensor,
        source_mask: torch.Tensor, 
        response_ids: torch.Tensor,
        response_ids_ref: torch.Tensor,
        rewards: torch.Tensor,
        response_mask: torch.Tensor
    ) -> Dict:
        """
        PPOCoder single step training main function
        
        Args:
            source_ids: Source code sequence
            source_mask: Source code mask
            response_ids: Generated response sequence
            response_ids_ref: Reference response sequence
            rewards: Reward tensor with shape (batch_size, seq_len), rewards placed at EOS positions
            response_mask: Mask for response sequences (1 for valid, 0 for padding)
            
        Returns:
            Training statistics dictionary
        """
        timing = {}
        t0 = time.time()
        
        # 1. Build complete input sequence
        full_input_ids = torch.cat([source_ids, response_ids], dim=1)
        # Use the provided response_mask instead of creating all-ones mask
        full_attention_mask = torch.cat([source_mask, response_mask], dim=1)
        
        # 2. Get current policy output
        timing['forward_start'] = time.time()
        
        with torch.no_grad():
            _, old_log_probs, old_values = self.forward_pass(
                full_input_ids, full_attention_mask
            )
            
            # Get reference model output
            with torch.amp.autocast('cuda', dtype=torch.bfloat16):
                ref_outputs = self.ref_model(
                    input_ids=full_input_ids,
                    attention_mask=full_attention_mask,
                    labels=None
                )
            ref_logits = ref_outputs[0]
            ref_log_probs = logprobs_from_logits(
                ref_logits[:, :-1], full_input_ids[:, 1:]
            )
        
        timing['forward_end'] = time.time()
        
        # 3. Calculate KL penalty and final rewards
        timing['reward_start'] = time.time()
        
        # Only calculate for response part
        source_len = source_ids.size(1)
        response_log_probs = old_log_probs[:, source_len-1:]
        response_ref_log_probs = ref_log_probs[:, source_len-1:]
        response_values = old_values[:, source_len-1:]

        # Directly use passed rewards (already correctly placed at EOS position)
        # Calculate KL penalty
        final_rewards, kl_penalties, kl_coef = self.compute_rewards_with_kl_penalty(
            response_log_probs, response_ref_log_probs, rewards, response_mask
        )
        
        timing['reward_end'] = time.time()
        
        # 4. PPO training step
        timing['train_start'] = time.time()
        
        # Reward mask check
        nonzero_reward_mask = final_rewards != 0
        total_nonzero_rewards = nonzero_reward_mask.sum().item()
        reward_mask_coverage = ((nonzero_reward_mask & (response_mask > 0.5)).sum().item() / total_nonzero_rewards 
                               if total_nonzero_rewards > 0 else 1.0)
        
        # GAE calculation and training
        advantages, returns = self.compute_gae_advantages(final_rewards, response_values, response_mask)
        train_stats = self.train_step(full_input_ids, full_attention_mask, final_rewards, 
                                     response_log_probs, response_values, response_mask)
        
        timing['train_end'] = time.time()
        
        # KL statistics and learning rate update
        per_seq_valid = response_mask.sum(dim=1).clamp(min=1)
        mean_kl = (((response_log_probs - response_ref_log_probs) * response_mask).sum(dim=1) / per_seq_valid).mean()
        
        # ðŸ”§ Fix KL controller update - dynamically adjust KL coefficient
        self.kl_controller.update(mean_kl.item(), self.step_count)
        
        # Statistics aggregation
        valid_advantages = advantages[response_mask.bool()]
        mask_sum = response_mask.sum()
        
        stats = {
            'ppo/mean_kl': mean_kl.item(), 'ppo/kl_coef': kl_coef,
            'ppo/kl_strategy': self.config.kl_penalty_strategy,  # Dynamic strategy identifier
            'ppo/kl_mode': self.config.kl_penalty_mode,  # KL penalty application mode
            'ppo/mean_reward': (rewards * response_mask).sum() / mask_sum,
            'ppo/mean_kl_penalty': (kl_penalties * response_mask).sum() / mask_sum,
            'ppo/mean_final_reward': (final_rewards * response_mask).sum() / mask_sum,
            'ppo/reward_mask_coverage': reward_mask_coverage,
            'ppo/advantages_raw_mean': (advantages * response_mask).sum() / mask_sum,
            'ppo/advantages_raw_std': valid_advantages.std().item() if len(valid_advantages) > 1 else 0.0,
            'ppo/advantages_normalized_mean': train_stats.get('normalized_advantages_mean', 0.0),
            'ppo/advantages_normalized_std': train_stats.get('normalized_advantages_std', 0.0),
            'ppo/advantages_mean': (advantages * response_mask).sum() / mask_sum,
            'ppo/returns_mean': (returns * response_mask).sum() / mask_sum,
            **{f'ppo/{k}': v.item() if hasattr(v, 'item') else v for k, v in train_stats.items()},
            **{f'time/ppo/{k}': v for k, v in timing.items()}
        }

        # Supplement loss keys and explained variance
        stats.update({
            'ppo/loss/total': torch.tensor(float(stats.get('ppo/total_loss', 0.0))),
            'ppo/loss/policy': torch.tensor(float(stats.get('ppo/policy_loss', 0.0))),
            'ppo/loss/value': torch.tensor(float(stats.get('ppo/value_loss', 0.0)))
        })
        
        if 'returns_var' in train_stats and train_stats['returns_var'] > 0:
            explained_var = 1 - train_stats['value_loss'] / train_stats['returns_var']
            explained_var_value = explained_var.item() if hasattr(explained_var, 'item') else explained_var
            stats['ppo/explained_variance'] = explained_var_value
            stats['ppo/val/var_explained'] = torch.tensor(float(explained_var_value))
        
        # Finalization
        timing['total'] = time.time() - t0
        stats['time/ppo/total'] = timing['total']
        
        # EOS advantage metrics
        for key in ['debug/advantage_at_eos_before_whiten', 'debug/advantage_at_eos_after_whiten', 'debug/advantage_at_eos_compression_ratio']:
            if key in train_stats:
                stats[key] = train_stats[key]
        
        torch.cuda.empty_cache()
        self.step_count += 1
        return stats



# Backward compatible alias
PPOTrainer = PPOCoderTrainer


def create_ppocoder_trainer(model, ref_model, learning_rate: float = 1e-5, init_kl_coef: float = 0.1, kl_target: float = 1.0, **kwargs) -> PPOCoderTrainer:
    """Create PPOCoder trainer"""
    config = PPOConfig(lr=learning_rate, init_kl_coef=init_kl_coef, kl_target=kl_target, **kwargs)
    return PPOCoderTrainer(model, ref_model, config)
