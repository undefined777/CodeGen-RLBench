# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02-ppo.ipynb (unless otherwise specified).

__all__ = ['AdaptiveKLController', 'FixedKLController', 'PPOTrainer']

# Cell
import numpy as np
import torch.nn.functional as F
from torch.optim import Adam, AdamW
import torch.optim as optim
import torch
import collections
import time
import random
import logging
from transformers import RobertaTokenizer
from utils import (logprobs_from_logits,
                         whiten,
                         clip_by_value,
                         entropy_from_logits,
                         flatten_dict,
                         average_torch_dicts,
                         stats_to_np,
                         stack_dicts,
                         add_suffix)
from mem import log_mem, mem_guard


class AdaptiveKLController:
    def __init__(self, init_kl_coef, target, horizon):
        self.value = init_kl_coef
        self.target = target
        self.horizon = horizon

    def update(self, current, n_steps):
        target = self.target
        proportional_error = np.clip(current / target - 1, -0.2, 0.2)
        mult = 1 + proportional_error * n_steps / self.horizon
        self.value *= mult

        
class FixedKLController:
    def __init__(self, kl_coef):
        self.value = kl_coef

    def update(self, current, n_steps):
        pass

    
class PPOTrainer:

    default_params = {
        "lr": 1e-5,
        "adap_kl_ctrl": True, 
        "init_kl_coef": 100, 
        "target": 6,
        "horizon":10000,
        "gamma":1,
        "lam":0.95,
        "cliprange": .2,
        "cliprange_value":.2,
        "vf_coef":0.5,  # ğŸ”§ å¢åŠ value functionç³»æ•°ï¼Œä»0.1æ”¹ä¸º0.5
        "batch_size": 48,
        "forward_batch_size": 16,
        "ppo_epochs": 4,
        "device": torch.device("cuda"),
        'adam_eps': 1e-8,
        'eos_token_id': 151645  # Qwen2.5-Coderçš„EOS token ID: <|im_end|>
    }

    def __init__(self, model, ref_model, **ppo_params):
        self.ppo_params = self.default_params
        self.ppo_params.update(ppo_params)

        self.ref_model = ref_model
        self.model = model
        self.optimizer = AdamW(model.parameters(), lr=self.ppo_params['lr'], eps=self.ppo_params['adam_eps'])
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer = self.optimizer, factor =1. / np.cbrt(2), patience= 100, verbose = True)
        self.metric = 0
  
        if self.ppo_params['adap_kl_ctrl']:
            self.kl_ctl = AdaptiveKLController(self.ppo_params['init_kl_coef'],
                                               self.ppo_params['target'],
                                               self.ppo_params['horizon'])
        else:
            self.kl_ctl = FixedKLController(self.ppo_params['init_kl_coef'])


    def step(self, source_ids, source_mask, response_ids, response_ids_ref, scores):
        """Single PPO optimisation step (batched version)."""
        bs = source_ids.size(0)
        timing, t0 = dict(), time.time()

        # 1) Forward pass over policy / reference
        t = time.time()
        log_mem("batched_forward_pass")
        logprobs, ref_logprobs, values = self.batched_forward_pass(
            source_ids, source_mask, response_ids
        )
        log_mem("after batched_forward_pass")
        timing["time/ppo/forward_pass"] = time.time() - t

        # 2) Compute rewards
        t = time.time()
        rewards, non_score_reward, kl_coef = self.compute_rewards(
            scores, logprobs, ref_logprobs
        )
        timing["time/ppo/compute_rewards"] = time.time() - t

        # 3) Optimise on *minibatches* sampled from the whole batch
        t = time.time()
        all_stats = []
        perm = torch.randperm(bs, device=source_ids.device)  # shuffle indices
        mb_size = self.ppo_params.get("minibatch_size", bs)
        print(f"mb_size: {mb_size}")
        log_mem("before train_minibatch")
        for start in range(0, bs, mb_size):
            idx = perm[start : start + mb_size]
            stats = self.train_minibatch(
                logprobs[idx],
                values[idx],
                rewards[idx],
                source_ids[idx],
                source_mask[idx],
                response_ids[idx],
                response_ids_ref[idx],
            )
            all_stats.append(stats)

        timing["time/ppo/optimize_step"] = time.time() - t

        # 4) Aggregate & record statistics (ä¿æŒåŸé€»è¾‘)
        train_stats = stack_dicts(all_stats)
        train_stats["policy/advantages"] = torch.flatten(
            train_stats["policy/advantages"]
        ).unsqueeze(0)
        train_stats["policy/ratio"] = torch.flatten(train_stats["policy/ratio"]).unsqueeze(0)

        stats = self.record_step_stats(
            scores=scores,
            logprobs=logprobs,
            ref_logprobs=ref_logprobs,
            non_score_reward=non_score_reward,
            train_stats=train_stats,
            kl_coef=kl_coef,
            response_ids=response_ids,
        )
        stats = stats_to_np(stats)
        timing['time/ppo/calc_stats'] = time.time()-t

        self.kl_ctl.update(stats['objective/kl'], self.ppo_params['batch_size'])

        timing['time/ppo/total'] = time.time()-t0
        stats.update(timing)
        log_mem("after optimizer.step")

        torch.cuda.empty_cache()   # å…³é”®ï¼šé‡Šæ”¾æœªç”¨ç¼“å­˜
        log_mem("after empty_cache")
        return stats


    def batched_forward_pass(self, source_ids, source_mask, response_ids):
        """
        Compute model & ref_model logprobs only over the *response* tokens.
        We must feed the *concatenated* [source | response] sequence to the model
        so that response logprobs are conditioned on the source prompt.
        """
        # æ„é€ æ‹¼æ¥åºåˆ—
        bs = source_ids.size(0)
        device = source_ids.device
        resp_mask = torch.ones_like(response_ids, dtype=source_mask.dtype, device=device)
        full_input = torch.cat([source_ids, response_ids], dim=1)          # [B, S+R]
        full_mask  = torch.cat([source_mask, resp_mask], dim=1)            # [B, S+R]

        with torch.no_grad():
            logits, _, values_full = self.model(input_ids=full_input,
                                               attention_mask=full_mask,
                                               labels=None)  # ä¸è®¡ç®—LM loss
            ref_logits, _, _ = self.ref_model(input_ids=full_input,
                                              attention_mask=full_mask,
                                              labels=None)

        # æˆ‘ä»¬åªéœ€è¦é¢„æµ‹ response token çš„logprobã€‚
        # æ¨¡å‹ logits[:, :-1] å¯¹åº” full_input[:, 1:]
        S = source_ids.size(1)
        # å¯¹ç¬¬ä¸€ä¸ªresponse tokençš„é¢„æµ‹æ¥è‡ªä½ç½® S-1
        logits_resp = logits[:, S-1:-1, :]     # é•¿åº¦ = R
        ref_logits_resp = ref_logits[:, S-1:-1, :]

        logprobs = logprobs_from_logits(logits_resp, response_ids).detach()
        ref_logprobs = logprobs_from_logits(ref_logits_resp, response_ids).detach()

        # å€¼å‡½æ•°ï¼šå–ä¸logprobså¯¹é½çš„tokenä½ç½®ï¼ˆé¢„æµ‹tokenæ‰€åœ¨ä½ç½®ï¼Œå³S-1...S+R-2ï¼‰
        values = values_full[:, S-1:-1].detach()
        return logprobs, ref_logprobs, values


    def train_minibatch(self, logprobs, values, rewards, source_ids, source_mask, response_ids,response_ids_ref): 
        """Train one PPO minibatch"""
        log_mem("before loss")
        loss_p, loss_v, train_stats  = self.loss(logprobs, values, rewards, source_ids, source_mask, response_ids)
        log_mem("after loss")
        loss = loss_p + loss_v 
        self.optimizer.zero_grad()
        with mem_guard("backward"):
            loss.backward()
        log_mem("after backward")
        print(torch.cuda.memory_summary(device=0, abbreviated=True)[:4000])
        self.optimizer.step()
        self.scheduler.step(self.metric)
        return train_stats


    def compute_rewards(self, scores, logprobs, ref_logprobs):
        """Compute per token rewards from scores and KL-penalty."""
        kl = logprobs - ref_logprobs
        non_score_reward = -self.kl_ctl.value * kl
        rewards = non_score_reward.clone().detach() 
        
        # è®°å½•å¥–åŠ±ä¿¡æ¯ç”¨äºè°ƒè¯•å’Œç›‘æ§
        logger = logging.getLogger(__name__)
        logger.debug(f'KL reward mean: {rewards.mean(axis=-1)}')
        
        rewards += scores
        logger.debug(f'Score reward sum: {scores.sum(axis=-1)}')
        
        return rewards, non_score_reward, self.kl_ctl.value


    def loss(self, old_logprobs, values, rewards, source_ids, source_mask, response_ids):
        lastgaelam = 0
        adv_rev = []
        T = response_ids.size(1)
        for t in reversed(range(T)):
            next_val = values[:, t + 1] if t < T - 1 else 0.0
            delta = rewards[:, t] + self.ppo_params["gamma"] * next_val - values[:, t]
            lastgaelam = delta + self.ppo_params["gamma"] * self.ppo_params["lam"] * lastgaelam
            adv_rev.append(lastgaelam)
        advantages = torch.stack(adv_rev[::-1]).transpose(0, 1)

        returns = advantages + values
        advantages = whiten(advantages)
        advantages = advantages.detach()

        # ä¸ batched_forward_pass ç›¸åŒï¼šæ‹¼æ¥åºåˆ—ï¼Œæ‰‹åŠ¨å¯¹é½ response
        device = source_ids.device
        resp_mask = torch.ones_like(response_ids, dtype=source_mask.dtype, device=device)
        full_input = torch.cat([source_ids, response_ids], dim=1)
        full_mask  = torch.cat([source_mask, resp_mask], dim=1)
        logits, _, vpred_full = self.model(input_ids=full_input,
                                           attention_mask=full_mask,
                                           labels=None)
        S = source_ids.size(1)
        logits_resp = logits[:, S-1:-1, :]
        logprob = logprobs_from_logits(logits_resp, response_ids)
        vpred = vpred_full[:, S-1:-1]
        vpredclipped = clip_by_value(vpred,
                                     values - self.ppo_params["cliprange_value"],
                                     values + self.ppo_params["cliprange_value"])

        vf_losses1 = (vpred - returns)**2
        vf_losses2 = (vpredclipped - returns)**2
        vf_loss = .5 * torch.mean(torch.max(vf_losses1, vf_losses2))
        vf_clipfrac =  torch.mean(torch.gt(vf_losses2, vf_losses1).double())

        ratio = torch.exp(logprob - old_logprobs)
        pg_losses = -advantages * ratio
        pg_losses2 = -advantages * torch.clamp(ratio,
                                               1.0 - self.ppo_params['cliprange'],
                                               1.0 + self.ppo_params['cliprange'])
        pg_loss = torch.mean(torch.max(pg_losses, pg_losses2))
        pg_clipfrac = torch.mean(torch.gt(pg_losses2, pg_losses).double())

        loss = pg_loss + self.ppo_params['vf_coef'] * vf_loss

        entropy = torch.mean(entropy_from_logits(logits))
        approxkl = .5 * torch.mean((logprob - old_logprobs)**2)

        policykl = torch.mean(logprob - old_logprobs)
        return_mean, return_var = torch.mean(returns), torch.var(returns)
        value_mean, value_var = torch.mean(values), torch.var(values)

        stats = dict(
            loss=dict(policy=pg_loss, value=vf_loss, total=loss),
            policy=dict(entropy=entropy.mean(), approxkl=approxkl,policykl=policykl, clipfrac=pg_clipfrac,
                        advantages=advantages, advantages_mean=torch.mean(advantages), ratio=ratio),
            returns=dict(mean=return_mean, var=return_var),
            val=dict(vpred=torch.mean(vpred), error=torch.mean((vpred - returns) ** 2),
                     clipfrac=vf_clipfrac, mean=value_mean, var=value_var),)

        return pg_loss, self.ppo_params['vf_coef'] * vf_loss, flatten_dict(stats)

    def record_step_stats(self, kl_coef, **data):
        """Record training step statistics (normalized, 0D-safe)."""
        logprobs      = data['logprobs']
        ref_logprobs  = data['ref_logprobs']
        non_score_rwd = data['non_score_reward']
        train_stats   = data.get('train_stats', {})

        # KL per token
        kl = logprobs - ref_logprobs                          # [B,T]
        mean_kl = kl.mean().detach().float()                  # scalar

        # Entropy: prefer true policy entropy computed in loss()
        if 'policy/entropy' in train_stats:
            mean_entropy = train_stats['policy/entropy'].mean().detach().float()
        else:
            # crude fallback
            mean_entropy = (-logprobs).mean().detach().float()

        # Non-score reward (e.g., -kl term) token-level mean
        mean_non_score_reward = non_score_rwd.mean().detach().float()

        stats = {
            'objective/kl': mean_kl,
            'objective/kl_dist': kl.detach(),                 # keep full dist (NOT scalar)
            'objective/logprobs': logprobs.detach(),
            'objective/ref_logprobs': ref_logprobs.detach(),
            'objective/kl_coef': torch.as_tensor(kl_coef).float(),
            'objective/entropy': mean_entropy,
            'ppo/mean_non_score_reward': mean_non_score_reward,
        }

        # fold in train_stats (policy loss, value loss, returns, etc.)
        for k, v in train_stats.items():
            # æœ‰äº› v å·²ç»æ˜¯æ ‡é‡ï¼Œæœ‰äº›æ˜¯å¼ é‡ï¼›ç»Ÿä¸€åš meanâ†’float
            stats[f'ppo/{k}'] = torch.as_tensor(v).mean().detach().float()

        # å…¼å®¹æ—§æŒ‡æ ‡ï¼švar_explained = 1 - error/returns_var
        if 'ppo/val/error' in stats and 'ppo/returns/var' in stats:
            stats['ppo/val/var_explained'] = (
                1.0 - stats['ppo/val/error'] / (stats['ppo/returns/var'] + 1e-8)
            )

        return stats
