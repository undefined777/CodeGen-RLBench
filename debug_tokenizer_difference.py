#!/usr/bin/env python3
"""
è°ƒè¯•tokenizerå·®å¼‚çš„è„šæœ¬
æ£€æŸ¥æœ¬åœ°å’ŒæœåŠ¡å™¨ç¯å¢ƒçš„apply_chat_templateè¡Œä¸º
"""

import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import os

def test_tokenizer_behavior():
    """æµ‹è¯•tokenizerçš„apply_chat_templateè¡Œä¸º"""
    
    # æ¨¡æ‹Ÿæ•°æ®
    system_message = "You are a helpful assistant for code translation. You specialize in translating Java code to C++ code while maintaining functionality and best practices."
    user_message = "Translate the following Java code to C++:\n\n```java\nclass Node { int data ; Node next ; Node ( int d ) { data = d ; next = null ; } } class LinkedList { Node head ; void push ( int new_data ) { Node new_node = new Node ( new_data ) ; new_node . next = head ; head = new_node ; } }```"
    
    print("ğŸ” ç¯å¢ƒä¿¡æ¯:")
    print(f"   Transformersç‰ˆæœ¬: {torch.__version__}")
    print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    model_path = "/home/cxy/Qwen2.5-Coder/finetuning/sft/checkpoints/qwen0.5b-lr5e-5-wr10-wd0.0-bsz1024-maxlen1280"
    print(f"\nğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"   è·¯å¾„å­˜åœ¨: {os.path.exists(model_path)}")
    
    # æ£€æŸ¥tokenizeré…ç½®æ–‡ä»¶
    tokenizer_config_path = os.path.join(model_path, "tokenizer_config.json")
    print(f"   tokenizer_config.jsonå­˜åœ¨: {os.path.exists(tokenizer_config_path)}")
    
    if os.path.exists(tokenizer_config_path):
        with open(tokenizer_config_path, 'r') as f:
            config = json.load(f)
        print(f"   chat_templateå­˜åœ¨: {'chat_template' in config}")
        if 'chat_template' in config:
            print(f"   chat_templateé•¿åº¦: {len(config['chat_template'])}")
    
    try:
        # åŠ è½½tokenizer
        print(f"\nğŸ”§ åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        print(f"   hasattr(tokenizer, 'apply_chat_template'): {hasattr(tokenizer, 'apply_chat_template')}")
        print(f"   tokenizer.chat_templateå­˜åœ¨: {hasattr(tokenizer, 'chat_template')}")
        
        if hasattr(tokenizer, 'chat_template'):
            print(f"   chat_templateé•¿åº¦: {len(tokenizer.chat_template)}")
            print(f"   chat_templateå‰100å­—ç¬¦: {tokenizer.chat_template[:100]}...")
        
        # æµ‹è¯•apply_chat_template
        print(f"\nğŸ§ª æµ‹è¯•apply_chat_template:")
        
        # æ–¹æ³•1: æœ‰systemæ¶ˆæ¯
        messages_with_system = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message},
        ]
        
        try:
            result_with_system = tokenizer.apply_chat_template(
                messages_with_system, add_generation_prompt=True, tokenize=False
            )
            print(f"   âœ… æœ‰systemæ¶ˆæ¯ - æˆåŠŸ")
            print(f"   ç»“æœé•¿åº¦: {len(result_with_system)}")
            print(f"   ç»“æœå‰200å­—ç¬¦: {result_with_system[:200]}...")
            print(f"   åŒ…å«'system': {'system' in result_with_system}")
        except Exception as e:
            print(f"   âŒ æœ‰systemæ¶ˆæ¯ - å¤±è´¥: {e}")
        
        # æ–¹æ³•2: æ— systemæ¶ˆæ¯
        messages_without_system = [
            {"role": "user", "content": user_message},
        ]
        
        try:
            result_without_system = tokenizer.apply_chat_template(
                messages_without_system, add_generation_prompt=True, tokenize=False
            )
            print(f"   âœ… æ— systemæ¶ˆæ¯ - æˆåŠŸ")
            print(f"   ç»“æœé•¿åº¦: {len(result_without_system)}")
            print(f"   ç»“æœå‰200å­—ç¬¦: {result_without_system[:200]}...")
        except Exception as e:
            print(f"   âŒ æ— systemæ¶ˆæ¯ - å¤±è´¥: {e}")
        
        # æ–¹æ³•3: ç›´æ¥ä½¿ç”¨åŸå§‹å†…å®¹
        print(f"\nğŸ“ ç›´æ¥ä½¿ç”¨åŸå§‹å†…å®¹:")
        print(f"   åŸå§‹user_messageé•¿åº¦: {len(user_message)}")
        print(f"   åŸå§‹user_messageå‰200å­—ç¬¦: {user_message[:200]}...")
        
        # å¯¹æ¯”ç»“æœ
        if 'result_with_system' in locals() and 'result_without_system' in locals():
            print(f"\nğŸ” ç»“æœå¯¹æ¯”:")
            print(f"   æœ‰system vs æ— systemé•¿åº¦å·®å¼‚: {len(result_with_system) - len(result_without_system)}")
            print(f"   æœ‰system vs åŸå§‹å†…å®¹é•¿åº¦å·®å¼‚: {len(result_with_system) - len(user_message)}")
            
    except Exception as e:
        print(f"âŒ åŠ è½½tokenizerå¤±è´¥: {e}")

if __name__ == "__main__":
    test_tokenizer_behavior() 