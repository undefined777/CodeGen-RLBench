#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„PPOä»£ç ç”Ÿæˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒç¨‹åº

ä¸»è¦åŠŸèƒ½ï¼š
1. ä»£ç ç¿»è¯‘ä»»åŠ¡çš„PPOè®­ç»ƒ - ä¸“ä¸ºQwen2.5-Coderè®¾è®¡
2. æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€å¯¹
3. åŸºäºç¼–è¯‘æˆåŠŸç‡å’Œä»£ç ç»“æ„çš„å¥–åŠ±è®¡ç®—
4. è‡ªé€‚åº”KLæ§åˆ¶å’Œç­–ç•¥è£å‰ª
5. è¯¦ç»†çš„è®­ç»ƒç›‘æ§å’Œæ—¥å¿—è®°å½•

ä½œè€…ï¼šAI Assistant
ç‰ˆæœ¬ï¼š2.0 - Qwenä¸“ç”¨ç‰ˆæœ¬
"""

import os
import sys
import torch
import numpy as np
import datetime
import argparse
import logging
import re
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from tqdm import tqdm
import json

# é¡¹ç›®å†…éƒ¨å¯¼å…¥
from code_parser import (DFG_python, DFG_java, DFG_ruby, DFG_go, 
                        DFG_php, DFG_javascript, DFG_csharp)
from code_parser import (tree_to_token_index, tree_to_token_nodes,
                        index_to_code_token, tree_to_variable_index, 
                        detokenize_code)
from tree_sitter import Language, Parser
from reward import remove_special_tokens, tree_sitter_full_compile, get_reward
from torch.utils.data import DataLoader, TensorDataset
from model import respond_to_batch, QwenCoderHeadWithValueModelLocal
from transformers import AutoTokenizer
from ppo import PPOTrainer
from utils import (extract_structure, Example, InputFeatures)
from code_prepro.lang_processors import (py_tokenizer, java_tokenizer, cpp_tokenizer,
                                        c_tokenizer, js_tokenizer, php_tokenizer, cs_tokenizer,
                                        py_detokenizer, java_detokenizer, cpp_detokenizer,
                                        c_detokenizer, js_detokenizer, php_detokenizer, cs_detokenizer)
from compiler.terminal_compiler import TerminalCompiler


def extract_code_from_qwen_response(response: str, target_lang: str = "cpp") -> str:
    """
    ä»Qwenæ¨¡å‹çš„å›å¤ä¸­æå–çº¯ä»£ç 
    
    Args:
        response: Qwenæ¨¡å‹çš„å®Œæ•´å›å¤
        target_lang: ç›®æ ‡è¯­è¨€ï¼Œç”¨äºåŒ¹é…ä»£ç å—
    
    Returns:
        æå–çš„çº¯ä»£ç å­—ç¬¦ä¸²
    """
    # è¯­è¨€åç§°æ˜ å°„ï¼Œæ”¯æŒä¸åŒçš„å˜ä½“
    lang_patterns = {
        'cpp': ['cpp', 'c++', 'cxx'],
        'java': ['java'],
        'python': ['python', 'py'],
        'javascript': ['javascript', 'js'],
        'c': ['c'],
        'php': ['php'],
        'c_sharp': ['csharp', 'c#', 'cs']
    }
    
    # è·å–ç›®æ ‡è¯­è¨€çš„æ‰€æœ‰å¯èƒ½æ¨¡å¼
    target_patterns = lang_patterns.get(target_lang, [target_lang])
    
    # å°è¯•åŒ¹é…ä»£ç å—
    for pattern in target_patterns:
        # åŒ¹é… ```lang\ncode\n``` æ ¼å¼ï¼Œè½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
        escaped_pattern = re.escape(pattern)
        code_match = re.search(rf'```{escaped_pattern}\s*\n(.*?)\n```', response, re.DOTALL | re.IGNORECASE)
        if code_match:
            return code_match.group(1).strip()
    
    # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šè¯­è¨€çš„ä»£ç å—ï¼Œå°è¯•åŒ¹é…é€šç”¨ä»£ç å—
    code_match = re.search(r'```\s*\n(.*?)\n```', response, re.DOTALL)
    if code_match:
        return code_match.group(1).strip()
    
    # å¦‚æœæ²¡æœ‰ä»£ç å—ï¼Œå°è¯•æå–"translation:"åçš„å†…å®¹
    translation_match = re.search(r'translation:\s*\n\n(.+)', response, re.DOTALL | re.IGNORECASE)
    if translation_match:
        return translation_match.group(1).strip()
    
    # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šè¿”å›å»é™¤å¸¸è§å‰ç¼€åçš„å†…å®¹
    response = response.strip()
    prefixes_to_remove = [
        "Here's the C++ translation:",
        "Here's the Java translation:",
        "Here's the Python translation:",
        "Here's the translation:",
        "Translation:",
        "```",
    ]
    
    for prefix in prefixes_to_remove:
        if response.startswith(prefix):
            response = response[len(prefix):].strip()
    
    # ç§»é™¤æœ«å°¾çš„ ```
    if response.endswith("```"):
        response = response[:-3].strip()
    
    return response


def read_qwen_examples(filename: str, args) -> List[Example]:
    """
    ä»Qwenæ ¼å¼çš„JSONLæ–‡ä»¶ä¸­è¯»å–è®­ç»ƒæ ·ä¾‹
    
    Args:
        filename: JSONLæ–‡ä»¶è·¯å¾„
        args: åŒ…å«è¯­è¨€é…ç½®çš„å‚æ•°å¯¹è±¡
    
    Returns:
        Exampleå¯¹è±¡åˆ—è¡¨
    """
    examples = []
    
    with open(filename, 'r', encoding='utf-8') as f:
        for idx, line in enumerate(f):
            if not line.strip():
                continue
                
            try:
                data = json.loads(line)
                messages = data.get('messages', [])
                
                # æŸ¥æ‰¾userå’Œassistantæ¶ˆæ¯
                user_message = None
                assistant_message = None
                
                for message in messages:
                    if message.get('role') == 'user':
                        user_message = message.get('content', '')
                    elif message.get('role') == 'assistant':
                        assistant_message = message.get('content', '')
                
                if not user_message or not assistant_message:
                    continue
                
                # ä»useræ¶ˆæ¯ä¸­æå–æºä»£ç  - ç”¨äºæ„å»ºExample.source
                source_code = extract_code_from_qwen_response(user_message, args.source_lang)
                
                # ä»assistantæ¶ˆæ¯ä¸­æå–ç›®æ ‡ä»£ç  - ç”¨äºæ„å»ºExample.target
                target_code = extract_code_from_qwen_response(assistant_message, args.target_lang)
                
                if not source_code or not target_code:
                    continue
                
                # Exampleçš„origå­—æ®µä¿å­˜å®Œæ•´çš„æ¶ˆæ¯ï¼Œç”¨äºtokenization
                # source_orig: å®Œæ•´çš„user promptï¼Œç”¨äºæ¨¡å‹è¾“å…¥
                # target_orig: å®Œæ•´çš„assistantå›å¤ï¼Œç”¨äºè®¡ç®—loss
                examples.append(
                    Example(
                        idx=idx,
                        source=source_code,  # çº¯ä»£ç ï¼Œç”¨äºæ˜¾ç¤º
                        target=target_code,  # çº¯ä»£ç ï¼Œç”¨äºæ˜¾ç¤º  
                        source_orig=user_message,  # å®Œæ•´promptï¼Œç”¨äºtokenization
                        target_orig=assistant_message  # å®Œæ•´å›å¤ï¼Œç”¨äºtokenization
                    )
                )
                
            except (json.JSONDecodeError, KeyError, IndexError) as e:
                print(f"è·³è¿‡ç¬¬{idx+1}è¡Œï¼Œè§£æé”™è¯¯: {e}")
                continue
    
    return examples


def convert_qwen_examples_to_features(examples, tokenizer, args, stage=None):
    """
    å°†Qwenæ ·ä¾‹è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥ç‰¹å¾
    ä¸“é—¨å¤„ç†å¯¹è¯æ ¼å¼çš„tokenization
    """
    features = []
    for example_index, example in enumerate(examples):
        # å¯¹äºQwenï¼Œæˆ‘ä»¬ä½¿ç”¨å®Œæ•´çš„å¯¹è¯æ¶ˆæ¯
        # source_origåŒ…å«å®Œæ•´çš„user prompt
        # target_origåŒ…å«å®Œæ•´çš„assistantå›å¤
        
        # å¯ä»¥ä½¿ç”¨tokenizerçš„chat templateï¼Œæˆ–è€…ç®€å•æ‹¼æ¥
        if hasattr(tokenizer, 'apply_chat_template'):
            # å°è¯•ä½¿ç”¨chat template
            try:
                messages = [
                    {"role": "user", "content": example.source_orig}
                ]
                source_text = tokenizer.apply_chat_template(
                    messages, add_generation_prompt=True, tokenize=False
                )
            except:
                # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹
                source_text = example.source_orig
        else:
            source_text = example.source_orig
            
        # tokenize source - ç›´æ¥ç¼–ç ï¼Œä¸éœ€è¦ç‰¹æ®Štoken
        source_ids = tokenizer.encode(source_text, max_length=args.max_source_length, 
                                     truncation=True, add_special_tokens=True)
        source_mask = [1] * len(source_ids)
        padding_length = args.max_source_length - len(source_ids)
        source_ids = [tokenizer.pad_token_id] * padding_length + source_ids  # âœ… left-padding
        source_mask = [0] * padding_length + source_mask  # âœ… left-padding
        
        # tokenize target
        if stage == "test":
            target_text = "None"
        else:
            target_text = example.target_orig
            
        target_ids = tokenizer.encode(target_text, max_length=args.max_target_length,
                                     truncation=True, add_special_tokens=True)
        target_mask = [1] * len(target_ids)
        padding_length = args.max_target_length - len(target_ids)
        target_ids = [tokenizer.pad_token_id] * padding_length + target_ids  # âœ… left-padding
        target_mask = [0] * padding_length + target_mask  # âœ… left-padding
        
        features.append(InputFeatures(
            example_index,
            source_ids,
            target_ids,
            source_mask,
            target_mask,
            example.target_orig))  # ä¿å­˜å®Œæ•´å›å¤ç”¨äºåç»­å¤„ç†
            
    return features


def create_reward_wrapper(original_get_reward):
    """
    åˆ›å»ºget_rewardçš„åŒ…è£…å™¨ï¼Œåœ¨è°ƒç”¨å‰æå–ä»£ç 
    """
    def get_reward_with_extraction(lang, code_ids=None, code_ref_ids=None, gold_ids=None, tokenizer=None):
        """
        è°ƒç”¨åŸå§‹get_rewardå‰ï¼Œå…ˆä»Qwenå“åº”ä¸­æå–ä»£ç 
        """
        # é¦–å…ˆè§£ç ä¸ºå®Œæ•´å“åº”
        code_ids_np = np.array(code_ids.cpu())
        eos_positions = []
        max_len = code_ids_np.shape[1]
        
        for id_seq in code_ids_np:
            if tokenizer.eos_token_id in id_seq:
                eos_positions.append((id_seq == tokenizer.eos_token_id).argmax())
            else:
                eos_positions.append(max_len)
        
        # è§£ç ä¸ºæ–‡æœ¬
        raw_responses = [
            tokenizer.decode(id_seq[:eos_pos], skip_special_tokens=True, clean_up_tokenization_spaces=False)
            for id_seq, eos_pos in zip(code_ids_np, eos_positions)
        ]
        raw_responses_ref = [
            tokenizer.decode(id_seq[:eos_pos], skip_special_tokens=True, clean_up_tokenization_spaces=False)
            for id_seq, eos_pos in zip(code_ref_ids.cpu().numpy(), eos_positions)
        ]
        raw_gold = [
            tokenizer.decode(id_seq[:eos_pos], skip_special_tokens=True, clean_up_tokenization_spaces=False)
            for id_seq, eos_pos in zip(gold_ids.cpu().numpy(), eos_positions)
        ]
        
        # æå–ä»£ç 
        extracted_codes = [extract_code_from_qwen_response(resp, lang) for resp in raw_responses]
        extracted_codes_ref = [extract_code_from_qwen_response(resp, lang) for resp in raw_responses_ref]
        extracted_codes_gold = [extract_code_from_qwen_response(resp, lang) for resp in raw_gold]
        
        
        # é‡æ–°ç¼–ç ä¸ºtoken ids
        extracted_ids = []
        extracted_ids_ref = []
        extracted_ids_gold = []
        
        # é¦–å…ˆç¼–ç æ‰€æœ‰ä»£ç ï¼Œæ”¶é›†é•¿åº¦ä¿¡æ¯
        all_tokens = []
        for code, code_ref, code_gold in zip(extracted_codes, extracted_codes_ref, extracted_codes_gold):
            code_tokens = tokenizer.encode(code, add_special_tokens=False)
            code_ref_tokens = tokenizer.encode(code_ref, add_special_tokens=False)
            code_gold_tokens = tokenizer.encode(code_gold, add_special_tokens=False)
            
            all_tokens.append((code_tokens, code_ref_tokens, code_gold_tokens))
        
        # è®¡ç®—æ‰€æœ‰ä»£ç çš„æœ€å¤§é•¿åº¦ï¼Œä½†ä¸è¶…è¿‡åŸå§‹tensorçš„é•¿åº¦
        all_lengths = [len(tokens) for tokens_group in all_tokens for tokens in tokens_group]
        max_code_len = min(max(all_lengths), max_len) if all_lengths else max_len
        
        # ä½¿ç”¨ç»Ÿä¸€é•¿åº¦è¿›è¡Œå¡«å……
        for code_tokens, code_ref_tokens, code_gold_tokens in all_tokens:
            # æˆªæ–­å’Œå¡«å……åˆ°ç»Ÿä¸€é•¿åº¦
            code_tokens = code_tokens[:max_code_len] + [tokenizer.pad_token_id] * (max_code_len - len(code_tokens))
            code_ref_tokens = code_ref_tokens[:max_code_len] + [tokenizer.pad_token_id] * (max_code_len - len(code_ref_tokens))
            code_gold_tokens = code_gold_tokens[:max_code_len] + [tokenizer.pad_token_id] * (max_code_len - len(code_gold_tokens))
            
            extracted_ids.append(code_tokens)
            extracted_ids_ref.append(code_ref_tokens)
            extracted_ids_gold.append(code_gold_tokens)
        
        # è½¬æ¢ä¸ºtensor
        extracted_tensor = torch.tensor(extracted_ids, device=code_ids.device)
        extracted_tensor_ref = torch.tensor(extracted_ids_ref, device=code_ids.device)
        extracted_tensor_gold = torch.tensor(extracted_ids_gold, device=code_ids.device)
        
        # è°ƒç”¨åŸå§‹get_rewardå‡½æ•°
        return original_get_reward(
            lang=lang,
            code_ids=extracted_tensor,
            code_ref_ids=extracted_tensor_ref,
            gold_ids=extracted_tensor_gold,
            tokenizer=tokenizer
        )
    
    return get_reward_with_extraction


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®æ•°æ®ç±» - Qwenä¸“ç”¨ç‰ˆæœ¬"""
    # è¯­è¨€é…ç½®
    source_lang: str
    target_lang: str
    
    # åˆ«åï¼Œç”¨äºå…¼å®¹æ—§ä»£ç 
    @property
    def l1(self):
        return self.source_lang
    
    @property
    def l2(self):
        return self.target_lang
    
    # æ¨¡å‹é…ç½®
    model_path: str
    max_source_length: int = 400
    max_target_length: int = 400
    
    # è®­ç»ƒé…ç½®
    train_batch_size: int = 16
    test_batch_size: int = 48
    train_epochs: int = 1000000
    learning_rate: float = 1e-5
    kl_coef: float = 0.05
    kl_target: float = 1.0
    vf_coef: float = 1e-3
    
    # ç”Ÿæˆé…ç½®
    action_space: int = 2  # top_k
    num_syn_samples: int = 5
    
    # è·¯å¾„é…ç½®
    data_path: str = None
    output_path: str = None
    baseline_output_path: str = None
    
    # è®¾å¤‡é…ç½®
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    
    # è¿è¡Œé…ç½®
    run_id: int = 1
    seed: int = 42


class CodeTranslationTrainer:
    """ä»£ç ç¿»è¯‘PPOè®­ç»ƒå™¨ - Qwenä¸“ç”¨ç‰ˆæœ¬"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.setup_logging()
        self.setup_device()
        self.setup_language_mappings()
        self.setup_parsers()
        self.setup_models()
        self.setup_data_loaders()
        self.setup_ppo_trainer()
        self.setup_training_stats()
        
        # åˆ›å»ºå¥–åŠ±å‡½æ•°åŒ…è£…å™¨
        self.get_reward_func = create_reward_wrapper(get_reward)
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = Path(self.config.output_path) / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / f"training_{self.config.run_id}.log"),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"è®­ç»ƒé…ç½®: {self.config}")
        
    def setup_device(self):
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if self.config.device == "cuda" and not torch.cuda.is_available():
            self.logger.warning("CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
            self.config.device = "cpu"
        
        torch.manual_seed(self.config.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(self.config.seed)
            
        self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.config.device}")
        
    def setup_language_mappings(self):
        """è®¾ç½®è¯­è¨€æ˜ å°„"""
        self.dir_dict = {
            'javascript': 'Javascript', 'java': 'Java', 'c_sharp': 'C#', 
            'php': 'PHP', 'python': 'Python', 'c': 'C', 'cpp': 'C++'
        }
        
    def setup_parsers(self):
        """è®¾ç½®ä»£ç è§£æå™¨"""
        self.dfg_function = {
            'python': DFG_python, 'java': DFG_java, 'php': DFG_php,
            'javascript': DFG_javascript, 'c_sharp': DFG_csharp,
            'c': DFG_csharp, 'cpp': DFG_csharp,
        }
        
        self.parsers = {}
        for lang in self.dfg_function:
            try:
                LANGUAGE = Language('code_parser/my-languages.so', lang)
                parser = Parser()
                parser.set_language(LANGUAGE)
                parser = [parser, self.dfg_function[lang]]
                self.parsers[lang] = parser
            except Exception as e:
                self.logger.warning(f"æ— æ³•åŠ è½½{lang}è§£æå™¨: {e}")
                
    def setup_models(self):
        """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨"""
        # è·å–æ¨¡å‹æ–‡ä»¶æ‰€åœ¨ç›®å½•
        self.model_dir = Path(self.config.model_path)
        
        # æ£€æŸ¥å¹¶å‡†å¤‡tokenizerå’Œé…ç½®æ–‡ä»¶
        self._check_model_files()
        
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°è®¾å¤‡: {self.config.device}")
        print(f"åŠ è½½æ¨¡å‹æ–‡ä»¶: {self.config.model_path}")
        
        # åˆå§‹åŒ–æ¨¡å‹ç»“æ„ï¼ˆä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼‰
        config_path = self.model_dir / 'config.json'
        
        # åŠ è½½ä¸»æ¨¡å‹
        self.model = QwenCoderHeadWithValueModelLocal(config_path)
        self.model.load_model_weights(self.config.model_path, self.config.device)
        self.model.to(self.config.device)
        
        # åŠ è½½å‚è€ƒæ¨¡å‹ï¼ˆå›ºå®šä¸å˜ï¼‰
        self.model_ref = QwenCoderHeadWithValueModelLocal(config_path)
        self.model_ref.load_model_weights(self.config.model_path, self.config.device)
        self.model_ref.to(self.config.device)
        
        # ä»æœ¬åœ°åŠ è½½tokenizer
        print("æ­£åœ¨ä»æœ¬åœ°åŠ è½½tokenizer...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_dir, 
            local_files_only=True,
            trust_remote_code=True,
            padding_side='left'  # Decoder-only æ¨¡å‹ä½¿ç”¨ left-padding
)
            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            print("tokenizerä»æœ¬åœ°åŠ è½½å®Œæˆï¼")
        except Exception as e:
            raise RuntimeError(f"ä»æœ¬åœ°åŠ è½½tokenizerå¤±è´¥: {e}")
        
        self.logger.info("æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½å®Œæˆ")
        
    def _check_model_files(self):
        """æ£€æŸ¥æ¨¡å‹å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        print("æ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
        
        # æ£€æŸ¥æ¨¡å‹æƒé‡æ–‡ä»¶
        if not os.path.exists(self.config.model_path):
            raise FileNotFoundError(f"æ¨¡å‹æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {self.config.model_path}")
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        required_files = [
            'config.json',
            'tokenizer.json',
            'vocab.json', 
            'merges.txt',
            'special_tokens_map.json'
        ]
        
        missing_files = []
        for file_name in required_files:
            file_path = self.model_dir / file_name
            if not file_path.exists():
                missing_files.append(file_name)
        
        if missing_files:
            raise FileNotFoundError(
                f"ç¼ºå°‘å¿…è¦çš„æ¨¡å‹æ–‡ä»¶: {missing_files}\n"
                f"è¯·ç¡®ä¿æ¨¡å‹ç›®å½• {self.model_dir} åŒ…å«æ‰€æœ‰å¿…è¦æ–‡ä»¶:\n"
                f"  - config.json (æ¨¡å‹é…ç½®)\n"
                f"  - tokenizer.json (åˆ†è¯å™¨é…ç½®)\n"
                f"  - vocab.json (è¯æ±‡è¡¨)\n"
                f"  - merges.txt (BPEåˆå¹¶è§„åˆ™)\n"
                f"  - special_tokens_map.json (ç‰¹æ®Štokenæ˜ å°„)\n"
                f"  - {Path(self.config.model_path).name} (æ¨¡å‹æƒé‡)"
            )
        
        print("âœ“ æ‰€æœ‰å¿…è¦æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
        
    def setup_data_loaders(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        # æ„å»ºæ•°æ®æ–‡ä»¶è·¯å¾„
        self.data_files = self._build_data_paths()
        
        # åŠ è½½æ•°æ®
        self.train_examples = read_qwen_examples(self.data_files['train'], self.config)
        self.dev_examples = read_qwen_examples(self.data_files['dev'], self.config)
        self.test_examples = read_qwen_examples(self.data_files['test'], self.config)
        
        # è½¬æ¢ä¸ºç‰¹å¾
        self.train_features = convert_qwen_examples_to_features(
            self.train_examples, self.tokenizer, self.config, stage='train'
        )
        self.dev_features = convert_qwen_examples_to_features(
            self.dev_examples, self.tokenizer, self.config, stage='train'
        )
        self.test_features = convert_qwen_examples_to_features(
            self.test_examples, self.tokenizer, self.config, stage='train'
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_dataloader = self._create_dataloader(
            self.train_features, self.config.train_batch_size, shuffle=True
        )
        self.dev_dataloader = self._create_dataloader(
            self.dev_features, self.config.train_batch_size, shuffle=False
        )
        self.test_dataloader = self._create_dataloader(
            self.test_features, self.config.test_batch_size, shuffle=False
        )
        
        self.logger.info(f"æ•°æ®åŠ è½½å®Œæˆ - è®­ç»ƒ: {len(self.train_features)}, "
                        f"éªŒè¯: {len(self.dev_features)}, æµ‹è¯•: {len(self.test_features)}")
        
    def _build_data_paths(self) -> Dict[str, str]:
        """æ„å»ºQwenæ ¼å¼æ•°æ®æ–‡ä»¶è·¯å¾„"""
        l1, l2 = self.config.source_lang, self.config.target_lang
        
        # å°è¯•ä¸åŒçš„è·¯å¾„ç»„åˆ
        possible_paths = [
            f"{self.config.data_path}/qwen/{self.dir_dict[l1]}-{self.dir_dict[l2]}/",
            f"{self.config.data_path}/qwen/{self.dir_dict[l2]}-{self.dir_dict[l1]}/",
            f"{self.config.data_path}/{self.dir_dict[l1]}-{self.dir_dict[l2]}/",  # å¤‡é€‰è·¯å¾„
            f"{self.config.data_path}/{self.dir_dict[l2]}-{self.dir_dict[l1]}/"   # å¤‡é€‰è·¯å¾„
        ]
        
        data_dir = None
        for path in possible_paths:
            if os.path.exists(path):
                data_dir = path
                break
                
        if data_dir is None:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°Qwenæ ¼å¼æ•°æ®ç›®å½•: {possible_paths}")
            
        return {
            'train': f"{data_dir}train.jsonl",
            'dev': f"{data_dir}val.jsonl",
            'test': f"{data_dir}test.jsonl"
        }
        
    def _create_dataloader(self, features: List[InputFeatures], 
                          batch_size: int, shuffle: bool = False) -> DataLoader:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        all_source_ids = torch.tensor([f.source_ids for f in features], dtype=torch.long)
        all_source_mask = torch.tensor([f.source_mask for f in features], dtype=torch.long)
        all_target_ids = torch.tensor([f.target_ids for f in features], dtype=torch.long)
        all_target_mask = torch.tensor([f.target_mask for f in features], dtype=torch.long)
        indices = torch.arange(len(features))
        
        dataset = TensorDataset(all_source_ids, all_source_mask, 
                               all_target_ids, all_target_mask, indices)
        
        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
        
    def setup_ppo_trainer(self):
        """è®¾ç½®PPOè®­ç»ƒå™¨"""
        ppo_config = {
            "batch_size": self.config.train_batch_size,
            'eos_token_id': self.tokenizer.eos_token_id,
            'lr': self.config.learning_rate,
            "adap_kl_ctrl": True,
            'init_kl_coef': self.config.kl_coef,
            "target": self.config.kl_target,
            "vf_coef": self.config.vf_coef
        }
        
        self.ppo_trainer = PPOTrainer(self.model, self.model_ref, **ppo_config)
        self.logger.info("PPOè®­ç»ƒå™¨è®¾ç½®å®Œæˆ")
        
    def setup_training_stats(self):
        """è®¾ç½®è®­ç»ƒç»Ÿè®¡"""
        self.training_stats = {
            'nsteps': 0,
            'total_nerrors': 0,
            'total_rewards': 0,
            'total_nnodes': 0,
            'total_nerrors_ref': 0,
            'total_nnodes_ref': 0,
            'total_seen': 0
        }
        
        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir = Path(self.config.output_path) / "results"
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
        self.checkpoint_dir = Path(self.config.output_path) / "checkpoints"
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        self.logger.info("å¼€å§‹è®­ç»ƒ...")
        
        for epoch in range(self.config.train_epochs):
            self.logger.info(f"å¼€å§‹ç¬¬ {epoch} è½®è®­ç»ƒ")
            
            # æ¯è½®è¿›è¡Œå¤šæ¬¡é‡‡æ ·
            for sample_idx in range(self.config.num_syn_samples):
                self._train_epoch(epoch, sample_idx)
                
            # ä¿å­˜æ¨¡å‹å’Œè¯„ä¼°
            self._save_checkpoint(epoch)
            self._evaluate(epoch)
            
    def _train_epoch(self, epoch: int, sample_idx: int):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        pbar = tqdm(self.train_dataloader, desc=f"Epoch {epoch}, Sample {sample_idx}")
        
        for batch_idx, batch in enumerate(pbar):
            # å¤„ç†æ‰¹æ¬¡æ•°æ®
            batch = tuple(t.to(self.config.device) for t in batch)
            source_ids, source_mask, target_ids, target_mask, _ = batch
            
            # ç”Ÿæˆä»£ç 
            response_ids = self._generate_code(source_ids, source_mask)
            response_ids_ref = self._generate_code_ref(source_ids, source_mask)
            
            # è®¡ç®—å¥–åŠ±
            reward, metrics = self._compute_reward(response_ids, response_ids_ref, target_ids)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self._update_stats(reward, metrics, len(source_ids))
            
            # PPOè®­ç»ƒæ­¥éª¤
            train_stats = self.ppo_trainer.step(
                source_ids, source_mask, response_ids, response_ids_ref, 
                reward.to(self.config.device)
            )
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_description(
                f"Epoch {epoch}, Sample {sample_idx}, "
                f"Avg Errors: {self.training_stats['total_nerrors']/self.training_stats['total_seen']:.5f}"
            )
            
            # è®°å½•è®­ç»ƒç»Ÿè®¡
            self._log_training_step(epoch, sample_idx, batch_idx, reward, metrics, train_stats)
            
            self.training_stats['nsteps'] += 1
            
    def _generate_code(self, source_ids: torch.Tensor, source_mask: torch.Tensor) -> torch.Tensor:
        """ç”Ÿæˆä»£ç """
        return torch.clone(respond_to_batch(
            self.model, source_ids, source_mask,
            max_target_length=self.config.max_target_length,
            top_k=self.config.action_space, top_p=1.0,
            tokenizer=self.tokenizer
        ).detach()[:, 1:])
        
    def _generate_code_ref(self, source_ids: torch.Tensor, source_mask: torch.Tensor) -> torch.Tensor:
        """ç”Ÿæˆå‚è€ƒä»£ç """
        return torch.clone(respond_to_batch(
            self.model_ref, source_ids, source_mask,
            max_target_length=self.config.max_target_length,
            top_k=self.config.action_space, top_p=1.0,
            tokenizer=self.tokenizer
        ).detach()[:, 1:])
        
    def _compute_reward(self, response_ids: torch.Tensor, response_ids_ref: torch.Tensor, 
                       target_ids: torch.Tensor) -> Tuple[torch.Tensor, Dict]:
        """è®¡ç®—å¥–åŠ±"""
        reward, mean_rate, mean_ast_match, mean_dfg_match, num_errors, num_errors_ref, num_nodes, num_nodes_ref = self.get_reward_func(
            lang=self.config.target_lang,
            code_ids=response_ids,
            code_ref_ids=response_ids_ref,
            gold_ids=target_ids,
            tokenizer=self.tokenizer
        )
        
        metrics = {
            'mean_rate': mean_rate,
            'mean_ast_match': mean_ast_match,
            'mean_dfg_match': mean_dfg_match,
            'num_errors': num_errors,
            'num_errors_ref': num_errors_ref,
            'num_nodes': num_nodes,
            'num_nodes_ref': num_nodes_ref
        }
        
        return reward, metrics
        
    def _update_stats(self, reward: torch.Tensor, metrics: Dict, batch_size: int):
        """æ›´æ–°è®­ç»ƒç»Ÿè®¡"""
        self.training_stats['total_rewards'] += float(sum(reward.sum(axis=-1).tolist()))
        self.training_stats['total_nerrors'] += sum(metrics['num_errors'])
        self.training_stats['total_nnodes'] += sum(metrics['num_nodes'])
        self.training_stats['total_nerrors_ref'] += sum(metrics['num_errors_ref'])
        self.training_stats['total_nnodes_ref'] += sum(metrics['num_nodes_ref'])
        self.training_stats['total_seen'] += batch_size
        
    def _log_training_step(self, epoch: int, sample_idx: int, batch_idx: int,
                          reward: torch.Tensor, metrics: Dict, train_stats: Dict):
        """è®°å½•è®­ç»ƒæ­¥éª¤"""
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_reward = float(sum(reward.sum(axis=-1).tolist())) / len(reward)
        avg_errors = sum(metrics['num_errors']) / len(metrics['num_errors'])
        avg_errors_ref = sum(metrics['num_errors_ref']) / len(metrics['num_errors_ref'])
        avg_nodes = sum(metrics['num_nodes']) / len(metrics['num_nodes'])
        avg_nodes_ref = sum(metrics['num_nodes_ref']) / len(metrics['num_nodes_ref'])
        
        # è®°å½•åˆ°CSVæ–‡ä»¶
        csv_line = [
            datetime.datetime.now().strftime("%H:%M:%S"),
            str(self.config.run_id),
            str(self.config.train_batch_size),
            str(self.config.max_source_length),
            str(self.config.max_target_length),
            str(self.config.learning_rate),
            str(epoch),
            str(self.training_stats['nsteps']),
            f"{avg_reward:.4f}",
            f"{avg_errors:.4f}",
            f"{avg_errors_ref:.4f}",
            f"{avg_nodes:.4f}",
            f"{avg_nodes_ref:.4f}",
            str(train_stats['objective/kl']),
            str(train_stats['objective/entropy']),
            str(train_stats['ppo/loss/total'].item()),
            str(train_stats['ppo/loss/policy'].item()),
            str(train_stats['ppo/loss/value'].item()),
            str(train_stats['ppo/policy/advantages_mean'].item()),
            str(train_stats['ppo/returns/mean'].item()),
            str(train_stats['ppo/val/mean'].item()),
            str(metrics['mean_rate']),
            str(metrics['mean_ast_match']),
            str(metrics['mean_dfg_match'])
        ]
        
        csv_file = self.results_dir / f"{self.config.source_lang}-{self.config.target_lang}.csv"
        with open(csv_file, 'a') as f:
            f.write(','.join(csv_line) + '\n')
            
    def _save_checkpoint(self, epoch: int):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model
        checkpoint_path = self.checkpoint_dir / f"pytorch_model_ep{epoch}.bin"
        torch.save(model_to_save.state_dict(), checkpoint_path)
        self.logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {checkpoint_path}")
        
    def _evaluate(self, epoch: int):
        """è¯„ä¼°æ¨¡å‹"""
        self.logger.info(f"å¼€å§‹ç¬¬ {epoch} è½®è¯„ä¼°")
        
        # è®­ç»ƒé›†è¯„ä¼°
        train_errors, train_errors_ref = self._evaluate_dataset(
            epoch, self.train_features, self.train_dataloader, 'train'
        )
        
        # æµ‹è¯•é›†è¯„ä¼°
        test_errors, test_errors_ref = self._evaluate_dataset(
            epoch, self.test_features, self.test_dataloader, 'test'
        )
        
        self.logger.info(f"Epoch {epoch} è¯„ä¼°ç»“æœ:")
        self.logger.info(f"  è®­ç»ƒé›† - æ¨¡å‹é”™è¯¯: {train_errors}, å‚è€ƒæ¨¡å‹é”™è¯¯: {train_errors_ref}")
        self.logger.info(f"  æµ‹è¯•é›† - æ¨¡å‹é”™è¯¯: {test_errors}, å‚è€ƒæ¨¡å‹é”™è¯¯: {test_errors_ref}")
        
    def _evaluate_dataset(self, epoch: int, features: List[InputFeatures], 
                         dataloader: DataLoader, prefix: str) -> Tuple[int, int]:
        """è¯„ä¼°æ•°æ®é›†"""
        pred_ids = []
        pred_ids_ref = []
        indices = []
        nerrors = 0
        nerrors_ref = 0
        
        with torch.no_grad():
            for batch in dataloader:
                batch = tuple(t.to(self.config.device) for t in batch)
                source_ids, source_mask, target_ids, target_mask, ind = batch
                
                # ç”Ÿæˆé¢„æµ‹
                preds = respond_to_batch(
                    self.model, source_ids, source_mask,
                    max_target_length=self.config.max_target_length,
                    top_k=self.config.action_space, top_p=1.0
                )[:, 1:]
                
                preds_ref = respond_to_batch(
                    self.model_ref, source_ids, source_mask,
                    max_target_length=self.config.max_target_length,
                    top_k=self.config.action_space, top_p=1.0
                )[:, 1:]
                
                # è®¡ç®—é”™è¯¯æ•°
                nerrors += sum(self.get_reward_func(
                    lang=self.config.target_lang,
                    code_ids=preds,
                    code_ref_ids=preds_ref,
                    gold_ids=target_ids,
                    tokenizer=self.tokenizer
                )[4])
                
                nerrors_ref += sum(self.get_reward_func(
                    lang=self.config.target_lang,
                    code_ids=preds_ref,
                    code_ref_ids=preds_ref,
                    gold_ids=target_ids,
                    tokenizer=self.tokenizer
                )[5])
                
                # ä¿å­˜é¢„æµ‹ç»“æœ
                pred_ids.extend(list(preds.cpu().numpy()))
                pred_ids_ref.extend(list(preds_ref.cpu().numpy()))
                indices.extend(list(ind.cpu().numpy()))
                
        # è§£ç å¹¶ä¿å­˜ç»“æœ
        self._save_predictions(epoch, prefix, pred_ids, pred_ids_ref, indices, features)
        
        return nerrors, nerrors_ref
        
    def _save_predictions(self, epoch: int, prefix: str, pred_ids: List, 
                         pred_ids_ref: List, indices: List, features: List[InputFeatures]):
        """ä¿å­˜é¢„æµ‹ç»“æœ"""
        # è§£ç é¢„æµ‹ç»“æœ
        raw_predictions = [
            self.tokenizer.decode(id, skip_special_tokens=True, clean_up_tokenization_spaces=False)
            for id in pred_ids
        ]
        raw_predictions_ref = [
            self.tokenizer.decode(id, skip_special_tokens=True, clean_up_tokenization_spaces=False)
            for id in pred_ids_ref
        ]
        
        # ä»Qwenå“åº”ä¸­æå–ä»£ç 
        predictions = [
            extract_code_from_qwen_response(pred, self.config.target_lang)
            for pred in raw_predictions
        ]
        predictions_ref = [
            extract_code_from_qwen_response(pred, self.config.target_lang)
            for pred in raw_predictions_ref
        ]
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        model_file = self.checkpoint_dir / f"{prefix}.model_ep{epoch}"
        ref_file = self.checkpoint_dir / f"{prefix}.model_ref_ep{epoch}"
        gold_file = self.checkpoint_dir / f"{prefix}.gold_ep{epoch}"
        
        with open(model_file, 'w') as f_model, \
             open(ref_file, 'w') as f_ref, \
             open(gold_file, 'w') as f_gold:
            
            for pred, ref, i in zip(predictions, predictions_ref, indices):
                f_model.write(pred + '\n')
                f_ref.write(ref + '\n')
                # å¯¹äºgoldï¼Œä¹Ÿéœ€è¦æå–ä»£ç 
                gold_code = extract_code_from_qwen_response(features[i].target, self.config.target_lang)
                f_gold.write(gold_code + '\n')


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Qwen2.5-Coder PPOä»£ç ç”Ÿæˆè®­ç»ƒç¨‹åº")
    
    # å¿…éœ€å‚æ•°
    parser.add_argument("--source_lang", required=True, type=str,
                       help="æºä»£ç è¯­è¨€")
    parser.add_argument("--target_lang", required=True, type=str,
                       help="ç›®æ ‡ä»£ç è¯­è¨€")
    parser.add_argument("--model_path", required=True, type=str,
                       help="Qwen2.5-Coderæ¨¡å‹è·¯å¾„")
    parser.add_argument("--data_path", required=True, type=str,
                       help="Qwenæ ¼å¼æ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--output_path", required=True, type=str,
                       help="è¾“å‡ºç›®å½•è·¯å¾„")
    
    # å¯é€‰å‚æ•°
    parser.add_argument("--max_source_length", default=400, type=int,
                       help="æœ€å¤§æºä»£ç é•¿åº¦")
    parser.add_argument("--max_target_length", default=400, type=int,
                       help="æœ€å¤§ç›®æ ‡ä»£ç é•¿åº¦")
    parser.add_argument("--train_batch_size", default=16, type=int,
                       help="è®­ç»ƒæ‰¹æ¬¡å¤§å°")
    parser.add_argument("--test_batch_size", default=48, type=int,
                       help="æµ‹è¯•æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--train_epochs", default=1000000, type=int,
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-5,
                       help="å­¦ä¹ ç‡")
    parser.add_argument("--kl_coef", type=float, default=0.05,
                       help="KLç³»æ•°")
    parser.add_argument("--kl_target", type=float, default=1.0,
                       help="KLç›®æ ‡å€¼")
    parser.add_argument("--vf_coef", type=float, default=1e-3,
                       help="ä»·å€¼å‡½æ•°ç³»æ•°")
    parser.add_argument("--action_space", default=2, type=int,
                       help="åŠ¨ä½œç©ºé—´å¤§å°ï¼ˆtop_kï¼‰")
    parser.add_argument("--num_syn_samples", default=5, type=int,
                       help="æ¯è½®é‡‡æ ·æ¬¡æ•°")
    parser.add_argument("--run_id", default=1, type=int,
                       help="è¿è¡ŒID")
    parser.add_argument("--seed", default=42, type=int,
                       help="éšæœºç§å­")
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    config = TrainingConfig(
        source_lang=args.source_lang,
        target_lang=args.target_lang,
        model_path=args.model_path,
        data_path=args.data_path,
        output_path=args.output_path,
        max_source_length=args.max_source_length,
        max_target_length=args.max_target_length,
        train_batch_size=args.train_batch_size,
        test_batch_size=args.test_batch_size,
        train_epochs=args.train_epochs,
        learning_rate=args.learning_rate,
        kl_coef=args.kl_coef,
        kl_target=args.kl_target,
        vf_coef=args.vf_coef,
        action_space=args.action_space,
        num_syn_samples=args.num_syn_samples,
        run_id=args.run_id,
        seed=args.seed
    )
    
    print("=" * 60)
    print("ğŸš€ Qwen2.5-Coder PPOä»£ç ç¿»è¯‘è®­ç»ƒç¨‹åº")
    print("=" * 60)
    print(f"ğŸ“ æºè¯­è¨€: {config.source_lang}")
    print(f"ğŸ¯ ç›®æ ‡è¯­è¨€: {config.target_lang}")
    print(f"ğŸ¤– æ¨¡å‹è·¯å¾„: {config.model_path}")
    print(f"ğŸ“‚ æ•°æ®è·¯å¾„: {config.data_path}")
    print(f"ğŸ’¾ è¾“å‡ºè·¯å¾„: {config.output_path}")
    print(f"ğŸ”§ è®¾å¤‡: {config.device}")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = CodeTranslationTrainer(config)
    trainer.train()


if __name__ == "__main__":
    main() 