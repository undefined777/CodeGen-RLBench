#!/usr/bin/env python3
"""
æ¨¡å‹è¯„ä¼°æµ‹è¯•è„šæœ¬
ç”¨äºæ¯”è¾ƒå¾®è°ƒå‰åæ¨¡å‹åœ¨ä»£ç ç¿»è¯‘ä»»åŠ¡ä¸Šçš„æ€§èƒ½

è¯„ä¼°ç»´åº¦ï¼š
1. ç¼–è¯‘é€šè¿‡ç‡è¯„åˆ†
2. ASTåŒ¹é…è¯„åˆ†  
3. DFGåŒ¹é…è¯„åˆ†
4. CodeBLEUè¯„åˆ†

æ•°æ®é›†ï¼šdata/qwen/Java-C++/val.jsonl
"""

import os
import json
import torch
import argparse
import numpy as np
from pathlib import Path
from tqdm import tqdm
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional
from transformers import AutoTokenizer

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from model import QwenCoderHeadWithValueModelLocal, respond_to_batch
from utils import (read_qwen_examples, convert_qwen_examples_to_features, 
                   extract_code_from_qwen_response, Example)
from reward import get_reward
from codebleu.calc_code_bleu import calc_code_bleu
from compiler.terminal_compiler import TerminalCompiler


@dataclass
class EvaluationConfig:
    """è¯„ä¼°é…ç½®"""
    # æ¨¡å‹è·¯å¾„
    model_before: str  # å¾®è°ƒå‰æ¨¡å‹è·¯å¾„
    model_after: str   # å¾®è°ƒåæ¨¡å‹è·¯å¾„
    
    # æ•°æ®é…ç½®
    data_path: str = "data/qwen/Java-Python/val.jsonl"
    source_lang: str = "java"
    target_lang: str = "python"
    
    # ç”Ÿæˆé…ç½®
    max_source_length: int = 700
    max_target_length: int = 700
    batch_size: int = 8
    top_k: int = 1
    top_p: float = 1
    temperature: float = 0
    do_sample: bool = False
    # è®¾å¤‡é…ç½®
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    
    # è¾“å‡ºé…ç½®
    output_dir: str = "evaluation_results"
    save_predictions: bool = True
    
    # McNemaræµ‹è¯•é…ç½®
    enable_mcnemar: bool = True  # æ˜¯å¦å¯ç”¨McNemaræµ‹è¯•
    mcnemar_alpha: float = 0.05  # McNemaræµ‹è¯•æ˜¾è‘—æ€§æ°´å¹³


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, config: EvaluationConfig):
        self.config = config
        self.setup_output_dir()
        self.load_data()
        self.setup_compiler()
        
    def setup_output_dir(self):
        """è®¾ç½®è¾“å‡ºç›®å½•"""
        self.output_dir = Path(self.config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
    def load_data(self):
        """åŠ è½½è¯„ä¼°æ•°æ®"""
        print(f"ğŸ“Š åŠ è½½æ•°æ®: {self.config.data_path}")
        
        # è‡ªåŠ¨æ¨æ–­è¯­è¨€å¯¹
        self.source_lang, self.target_lang = self._infer_languages_from_path(self.config.data_path)
        print(f"ğŸŒ æ¨æ–­ç¿»è¯‘ä»»åŠ¡: {self.source_lang} â†’ {self.target_lang}")
        
        # åˆ›å»ºä¸´æ—¶é…ç½®å¯¹è±¡ç”¨äºæ•°æ®åŠ è½½
        class TempArgs:
            def __init__(self, source_lang, target_lang):
                self.source_lang = source_lang
                self.target_lang = target_lang
                
        temp_args = TempArgs(self.source_lang, self.target_lang)
        self.examples = read_qwen_examples(self.config.data_path, temp_args)
        print(f"âœ… åŠ è½½äº† {len(self.examples)} ä¸ªæ ·æœ¬")
        
    def _infer_languages_from_path(self, data_path: str) -> tuple:
        """ä»æ•°æ®è·¯å¾„æ¨æ–­æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€"""
        if "Java-Python" in data_path:
            return "java", "python"
        elif "Java-C++" in data_path:
            return "java", "cpp"
        elif "C++-Python" in data_path:
            return "cpp", "python"
        else:
            # é»˜è®¤Java->Python
            return "java", "python"
        
    def setup_compiler(self):
        """è®¾ç½®ç¼–è¯‘å™¨"""
        # è¯­è¨€æ˜ å°„
        lang_mapping = {
            "cpp": "C++",
            "java": "Java", 
            "python": "Python",
            "c": "C",
            "php": "PHP",
            "c_sharp": "C#"
        }
        compiler_lang = lang_mapping.get(self.target_lang, "Python")
        self.compiler = TerminalCompiler(compiler_lang)
        print(f"ğŸ”§ è®¾ç½®ç¼–è¯‘å™¨: {self.target_lang} -> {compiler_lang}")
        
    def load_model_and_tokenizer(self, model_path: str):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_path}")
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True, 
            padding_side='right'
        )
        
        # åŠ è½½æ¨¡å‹
        model = QwenCoderHeadWithValueModelLocal(
            model_path, 
            torch_dtype=torch.bfloat16, 
            device=self.config.device
        )
        model.to(self.config.device)
        model.eval()
        
        return model, tokenizer
        
    def generate_predictions(self, model, tokenizer, examples: List[Example]) -> List[str]:
        """ç”Ÿæˆæ¨¡å‹é¢„æµ‹"""
        predictions = []
        
        # è½¬æ¢ä¸ºç‰¹å¾
        features = convert_qwen_examples_to_features(
            examples, tokenizer, self.config, stage='test'
        )
        
        # æ‰¹é‡ç”Ÿæˆ
        print(f"ğŸ”„ ç”Ÿæˆé¢„æµ‹ç»“æœ...")
        for i in tqdm(range(0, len(features), self.config.batch_size)):
            batch_features = features[i:i + self.config.batch_size]
            
            # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
            source_ids = torch.tensor([f.source_ids for f in batch_features], dtype=torch.long).to(self.config.device)
            source_mask = torch.tensor([f.source_mask for f in batch_features], dtype=torch.long).to(self.config.device)
            
            # ç”Ÿæˆä»£ç 
            with torch.no_grad():
                full_outputs = respond_to_batch(
                    model, source_ids, source_mask,
                    max_target_length=self.config.max_target_length,
                    top_k=self.config.top_k,
                    top_p=self.config.top_p,
                    tokenizer=tokenizer,
                    temperature=self.config.temperature,
                    do_sample=self.config.do_sample
                )
                
                # æå–ç”Ÿæˆçš„éƒ¨åˆ†
                generated_ids = full_outputs[:, source_ids.size(1):]
                
                # è§£ç 
                for gen_ids in generated_ids:
                    decoded = tokenizer.decode(gen_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
                    extracted_code = extract_code_from_qwen_response(decoded, self.target_lang)
                    predictions.append(extracted_code)
        
        return predictions
        
    def evaluate_compilation(self, predictions: List[str]) -> Dict:
        """è¯„ä¼°ç¼–è¯‘é€šè¿‡ç‡"""
        print("ğŸ”¨ è¯„ä¼°ç¼–è¯‘é€šè¿‡ç‡...")
        
        compilation_results = []
        success_count = 0
        
        for i, code in enumerate(tqdm(predictions)):
            try:
                # ç¼–è¯‘ä»£ç 
                compile_result = self.compiler.compile_code_string(code)
                success = compile_result[2] if len(compile_result) > 2 else False
                
                compilation_results.append({
                    'index': i,
                    'success': success,
                    'code': code,
                    'error': compile_result[1] if not success and len(compile_result) > 1 else None
                })
                
                if success:
                    success_count += 1
                    
            except Exception as e:
                compilation_results.append({
                    'index': i,
                    'success': False,
                    'code': code,
                    'error': str(e)
                })
        
        compile_rate = success_count / len(predictions) if predictions else 0.0
        
        return {
            'compile_rate': compile_rate,
            'success_count': success_count,
            'total_count': len(predictions),
            'details': compilation_results
        }
        
    def evaluate_codebleu(self, predictions: List[str], targets: List[str]) -> Dict:
        """è¯„ä¼°CodeBLEUæŒ‡æ ‡"""
        print("ğŸ“Š è¯„ä¼°CodeBLEUæŒ‡æ ‡...")
        
        # å‡†å¤‡keywordsç›®å½•
        keywords_dir = './codebleu/keywords/'
        if not os.path.exists(keywords_dir):
            keywords_dir = './CodeBLEU/keywords/'
            
        try:
            # è°ƒç”¨CodeBLEUè®¡ç®—
            result = calc_code_bleu([targets], predictions, self.target_lang, keywords_dir)
            
            return {
                'bleu': result[0],                    # BLEUåˆ†æ•°
                'weighted_bleu': result[1],           # åŠ æƒBLEUåˆ†æ•°  
                'ast_match': result[2],               # ASTåŒ¹é…åˆ†æ•°
                'dfg_match': result[3],               # DFGåŒ¹é…åˆ†æ•°
                'codebleu': result[4],                # ç»¼åˆCodeBLEUåˆ†æ•°
                'error': None
            }
            
        except Exception as e:
            print(f"âš ï¸ CodeBLEUè®¡ç®—å¤±è´¥: {e}")
            return {
                'bleu': 0.0,
                'weighted_bleu': 0.0,
                'ast_match': 0.0,
                'dfg_match': 0.0,
                'codebleu': 0.0,
                'error': str(e)
            }
    
    def evaluate_ast_dfg_individual(self, predictions: List[str], targets: List[str]) -> Dict:
        """å•ç‹¬è¯„ä¼°ASTå’ŒDFGåŒ¹é…åº¦ï¼ˆé€ä¸ªæ ·æœ¬ï¼‰"""
        print("ğŸŒ³ è¯„ä¼°ASTå’ŒDFGåŒ¹é…åº¦...")
        
        from codebleu.calc_code_bleu import calc_code_bleu
        
        ast_scores = []
        dfg_scores = []
        keywords_dir = './codebleu/keywords/'
        
        for pred, target in tqdm(zip(predictions, targets), total=len(predictions)):
            try:
                # å•ä¸ªæ ·æœ¬çš„CodeBLEUè®¡ç®—
                result = calc_code_bleu([[target]], [pred], self.target_lang, keywords_dir)
                ast_scores.append(result[2])
                dfg_scores.append(result[3])
            except Exception as e:
                ast_scores.append(0.0)
                dfg_scores.append(0.0)
        
        return {
            'ast_mean': np.mean(ast_scores),
            'ast_std': np.std(ast_scores),
            'ast_scores': ast_scores,
            'dfg_mean': np.mean(dfg_scores),
            'dfg_std': np.std(dfg_scores),
            'dfg_scores': dfg_scores
        }
    
    def calculate_mcnemar_compilation(self, before_compilation: List[bool], after_compilation: List[bool]) -> Dict:
        """
        ä½¿ç”¨McNemaræµ‹è¯•è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹çš„ç¼–è¯‘é€šè¿‡ç‡å·®å¼‚
        
        æ··æ·†çŸ©é˜µ:
        - n00: ä¸¤ä¸ªæ¨¡å‹éƒ½ç¼–è¯‘å¤±è´¥
        - n01: å¾®è°ƒå‰å¤±è´¥ï¼Œå¾®è°ƒåæˆåŠŸ (å…³é”®æŒ‡æ ‡)
        - n10: å¾®è°ƒå‰æˆåŠŸï¼Œå¾®è°ƒåå¤±è´¥ (å…³é”®æŒ‡æ ‡)  
        - n11: ä¸¤ä¸ªæ¨¡å‹éƒ½ç¼–è¯‘æˆåŠŸ
        
        é‡ç‚¹å…³æ³¨n01 vs n10çš„å·®å¼‚ï¼Œn01 > n10è¡¨ç¤ºå¾®è°ƒæœ‰æ•ˆ
        """
        print("ğŸ“Š è®¡ç®—McNemarç¼–è¯‘é€šè¿‡ç‡æµ‹è¯•...")
        
        if len(before_compilation) != len(after_compilation):
            raise ValueError("ç¼–è¯‘ç»“æœåˆ—è¡¨é•¿åº¦ä¸åŒ¹é…")
        
        # æ„å»ºæ··æ·†çŸ©é˜µ
        n00 = n01 = n10 = n11 = 0
        
        for before_success, after_success in zip(before_compilation, after_compilation):
            if not before_success and not after_success:
                n00 += 1
            elif not before_success and after_success:
                n01 += 1  # å¾®è°ƒå‰å¤±è´¥ï¼Œå¾®è°ƒåæˆåŠŸ
            elif before_success and not after_success:
                n10 += 1  # å¾®è°ƒå‰æˆåŠŸï¼Œå¾®è°ƒåå¤±è´¥
            else:  # before_success and after_success
                n11 += 1
        
        # è®¡ç®—McNemarç»Ÿè®¡é‡ (è¿ç»­æ€§æ ¡æ­£)
        if n01 + n10 > 0:
            mcnemar_statistic = (abs(n01 - n10) - 1) ** 2 / (n01 + n10)
            # è‡ªç”±åº¦ä¸º1çš„å¡æ–¹åˆ†å¸ƒ
            from scipy.stats import chi2
            p_value = 1 - chi2.cdf(mcnemar_statistic, df=1)
        else:
            mcnemar_statistic = 0.0
            p_value = 1.0
        
        # è®¡ç®—æ”¹è¿›æ•ˆæœ
        improvement_rate = (n01 - n10) / (n01 + n10) if (n01 + n10) > 0 else 0.0
        
        # åˆ¤æ–­ç»Ÿè®¡æ˜¾è‘—æ€§
        is_significant = p_value < self.config.mcnemar_alpha
        
        result = {
            'confusion_matrix': {
                'n00': n00,  # éƒ½å¤±è´¥
                'n01': n01,  # å‰å¤±è´¥åæˆåŠŸ (å…³é”®æŒ‡æ ‡)
                'n10': n10,  # å‰æˆåŠŸåå¤±è´¥ (å…³é”®æŒ‡æ ‡)
                'n11': n11   # éƒ½æˆåŠŸ
            },
            'mcnemar_statistic': mcnemar_statistic,
            'p_value': p_value,
            'is_significant': is_significant,
            'improvement_rate': improvement_rate,
            'interpretation': self._interpret_mcnemar_compilation(n01, n10, p_value, is_significant)
        }
        
        return result
    
    def _interpret_mcnemar_compilation(self, n01: int, n10: int, p_value: float, is_significant: bool) -> str:
        """è§£é‡ŠMcNemarç¼–è¯‘é€šè¿‡ç‡æµ‹è¯•ç»“æœ"""
        if n01 == 0 and n10 == 0:
            return "ä¸¤ä¸ªæ¨¡å‹ç¼–è¯‘è¡¨ç°å®Œå…¨ä¸€è‡´ï¼Œæ— å˜åŒ–"
        
        if n01 > n10:
            direction = "æ­£å‘"
            improvement = f"å¾®è°ƒåç¼–è¯‘æˆåŠŸç‡æå‡äº† {n01 - n10} ä¸ªæ ·æœ¬"
        elif n01 < n10:
            direction = "è´Ÿå‘"
            improvement = f"å¾®è°ƒåç¼–è¯‘æˆåŠŸç‡ä¸‹é™äº† {n10 - n01} ä¸ªæ ·æœ¬"
        else:
            direction = "æ— å˜åŒ–"
            improvement = "å¾®è°ƒå‰åç¼–è¯‘æˆåŠŸç‡ç›¸åŒ"
        
        significance = "ç»Ÿè®¡æ˜¾è‘—" if is_significant else "ç»Ÿè®¡ä¸æ˜¾è‘—"
        
        if is_significant:
            if n01 > n10:
                conclusion = "å¾®è°ƒæ•ˆæœæ˜¾è‘—ï¼Œæ¨¡å‹ç¼–è¯‘èƒ½åŠ›æœ‰å®è´¨æ€§æå‡"
            elif n01 < n10:
                conclusion = "å¾®è°ƒæ•ˆæœæ˜¾è‘—ï¼Œä½†æ¨¡å‹ç¼–è¯‘èƒ½åŠ›å‡ºç°å®è´¨æ€§ä¸‹é™"
            else:
                conclusion = "å¾®è°ƒæ•ˆæœæ˜¾è‘—ï¼Œä½†æ–¹å‘éœ€è¦è¿›ä¸€æ­¥åˆ†æ"
        else:
            conclusion = "å¾®è°ƒæ•ˆæœä¸æ˜¾è‘—ï¼Œéœ€è¦æ›´å¤šæ•°æ®æˆ–è°ƒæ•´è®­ç»ƒç­–ç•¥"
        
        return f"{direction}å˜åŒ–ï¼Œ{improvement}ã€‚{significance}ï¼Œ{conclusion}"
    
    def print_mcnemar_compilation_results(self, mcnemar_results: Dict):
        """æ‰“å°McNemarç¼–è¯‘é€šè¿‡ç‡æµ‹è¯•ç»“æœ"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š McNemarç¼–è¯‘é€šè¿‡ç‡æµ‹è¯•ç»“æœ")
        print(f"{'='*60}")
        
        cm = mcnemar_results['confusion_matrix']
        print(f"ğŸ” æ··æ·†çŸ©é˜µ:")
        print(f"   n00 (éƒ½å¤±è´¥): {cm['n00']}")
        print(f"   n01 (å‰å¤±è´¥åæˆåŠŸ): {cm['n01']} â† å…³é”®æŒ‡æ ‡")
        print(f"   n10 (å‰æˆåŠŸåå¤±è´¥): {cm['n10']} â† å…³é”®æŒ‡æ ‡")
        print(f"   n11 (éƒ½æˆåŠŸ): {cm['n11']}")
        print(f"")
        
        print(f"ğŸ“ˆ æ”¹è¿›åˆ†æ:")
        print(f"   n01 - n10 = {cm['n01']} - {cm['n10']} = {cm['n01'] - cm['n10']}")
        if cm['n01'] + cm['n10'] > 0:
            print(f"   æ”¹è¿›ç‡: {mcnemar_results['improvement_rate']:.4f}")
        print(f"")
        
        print(f"ğŸ“Š ç»Ÿè®¡æµ‹è¯•:")
        print(f"   McNemarç»Ÿè®¡é‡: {mcnemar_results['mcnemar_statistic']:.4f}")
        print(f"   på€¼: {mcnemar_results['p_value']:.6f}")
        print(f"   æ˜¾è‘—æ€§æ°´å¹³: Î± = {self.config.mcnemar_alpha}")
        print(f"   ç»Ÿè®¡æ˜¾è‘—: {'âœ… æ˜¯' if mcnemar_results['is_significant'] else 'âŒ å¦'}")
        print(f"")
        
        print(f"ğŸ’¡ ç»“æœè§£é‡Š:")
        print(f"   {mcnemar_results['interpretation']}")
        print(f"{'='*60}")
        
    def evaluate_model(self, model_path: str, model_name: str) -> Dict:
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        print(f"\n{'='*60}")
        print(f"ğŸ¯ è¯„ä¼°æ¨¡å‹: {model_name}")
        print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"{'='*60}")
        
        # åŠ è½½æ¨¡å‹
        model, tokenizer = self.load_model_and_tokenizer(model_path)
        
        # ç”Ÿæˆé¢„æµ‹
        predictions = self.generate_predictions(model, tokenizer, self.examples)
        
        # æå–ç›®æ ‡ä»£ç 
        targets = [example.target for example in self.examples]
        
        # è¯„ä¼°ç¼–è¯‘é€šè¿‡ç‡
        compilation_eval = self.evaluate_compilation(predictions)
        
        # è¯„ä¼°CodeBLEU
        codebleu_eval = self.evaluate_codebleu(predictions, targets)
        
        # è¯„ä¼°å•ç‹¬çš„ASTå’ŒDFG
        ast_dfg_eval = self.evaluate_ast_dfg_individual(predictions, targets)
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        if self.config.save_predictions:
            pred_file = self.output_dir / f"{model_name}_predictions.json"
            with open(pred_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'predictions': predictions,
                    'targets': targets,
                    'compilation_details': compilation_eval['details']
                }, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ é¢„æµ‹ç»“æœä¿å­˜åˆ°: {pred_file}")
        
        # æ¸…ç†æ˜¾å­˜
        del model, tokenizer
        torch.cuda.empty_cache()
        
        return {
            'model_name': model_name,
            'model_path': model_path,
            'compilation': compilation_eval,
            'codebleu': codebleu_eval,
            'ast_dfg': ast_dfg_eval,
            'sample_count': len(predictions)
        }
        
    def run_evaluation(self) -> Dict:
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print(f"\nğŸš€ å¼€å§‹æ¨¡å‹å¯¹æ¯”è¯„ä¼°")
        print(f"ğŸ“Š æ•°æ®é›†: {self.config.data_path}")
        print(f"ğŸ”„ æ ·æœ¬æ•°é‡: {len(self.examples)}")
        print(f"ğŸŒ ç¿»è¯‘æ–¹å‘: {self.source_lang} â†’ {self.target_lang}")
        
        results = {}
        
        # è¯„ä¼°å¾®è°ƒå‰æ¨¡å‹
        results['before'] = self.evaluate_model(self.config.model_before, "å¾®è°ƒå‰æ¨¡å‹")
        
        # è¯„ä¼°å¾®è°ƒåæ¨¡å‹  
        results['after'] = self.evaluate_model(self.config.model_after, "å¾®è°ƒåæ¨¡å‹")
        
        # æ‰§è¡ŒMcNemarç¼–è¯‘é€šè¿‡ç‡æµ‹è¯•
        if self.config.enable_mcnemar:
            print("\nğŸ“Š æ‰§è¡ŒMcNemarç¼–è¯‘é€šè¿‡ç‡æµ‹è¯•...")
            before_compilation = [detail['success'] for detail in results['before']['compilation']['details']]
            after_compilation = [detail['success'] for detail in results['after']['compilation']['details']]
            
            mcnemar_results = self.calculate_mcnemar_compilation(before_compilation, after_compilation)
            results['mcnemar_compilation'] = mcnemar_results
            
            # æ‰“å°McNemaræµ‹è¯•ç»“æœ
            self.print_mcnemar_compilation_results(mcnemar_results)
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self.generate_comparison_report(results)
        
        return results
        
    def generate_comparison_report(self, results: Dict):
        """ç”Ÿæˆå¯¹æ¯”è¯„ä¼°æŠ¥å‘Š"""
        print(f"\nğŸ“‹ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        
        before = results['before']
        after = results['after']
        
        # è®¡ç®—æ”¹è¿›å¹…åº¦
        def calc_improvement(before_val, after_val):
            if before_val == 0:
                return float('inf') if after_val > 0 else 0
            return ((after_val - before_val) / before_val) * 100
        
        # å‡†å¤‡æŠ¥å‘Šæ•°æ®
        report_data = {
            'evaluation_config': {
                'data_path': self.config.data_path,
                'source_lang': self.source_lang,
                'target_lang': self.target_lang,
                'sample_count': len(self.examples),
                'model_before': self.config.model_before,
                'model_after': self.config.model_after
            },
            'results': results,
            'comparison': {
                'compilation': {
                    'before': before['compilation']['compile_rate'],
                    'after': after['compilation']['compile_rate'],
                    'improvement': calc_improvement(
                        before['compilation']['compile_rate'],
                        after['compilation']['compile_rate']
                    )
                },
                'codebleu': {
                    'before': before['codebleu']['codebleu'],
                    'after': after['codebleu']['codebleu'],
                    'improvement': calc_improvement(
                        before['codebleu']['codebleu'],
                        after['codebleu']['codebleu']
                    )
                },
                'ast_match': {
                    'before': before['ast_dfg']['ast_mean'],
                    'after': after['ast_dfg']['ast_mean'],
                    'improvement': calc_improvement(
                        before['ast_dfg']['ast_mean'],
                        after['ast_dfg']['ast_mean']
                    )
                },
                'dfg_match': {
                    'before': before['ast_dfg']['dfg_mean'],
                    'after': after['ast_dfg']['dfg_mean'],
                    'improvement': calc_improvement(
                        before['ast_dfg']['dfg_mean'],
                        after['ast_dfg']['dfg_mean']
                    )
                }
            }
        }
        
        # æ·»åŠ McNemaræµ‹è¯•ç»“æœåˆ°æŠ¥å‘Š
        if 'mcnemar_compilation' in results:
            report_data['mcnemar_test'] = results['mcnemar_compilation']
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = self.output_dir / "evaluation_report.json"
        
        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œç¡®ä¿JSONåºåˆ—åŒ–
        def convert_numpy_types(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.bool_):
                return bool(obj)
            elif isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            else:
                return obj
        
        # è½¬æ¢æŠ¥å‘Šæ•°æ®
        serializable_report = convert_numpy_types(report_data)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_report, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆç®€æ´çš„æ–‡æœ¬æŠ¥å‘Š
        self.print_summary_report(report_data)
        
        # ä¿å­˜ç®€æ´æŠ¥å‘Š
        summary_file = self.output_dir / "evaluation_summary.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(self.format_summary_report(report_data))
        
        print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Š: {report_file}")
        print(f"ğŸ“‹ ç®€æ´æŠ¥å‘Š: {summary_file}")
        
    def print_summary_report(self, report_data: Dict):
        """æ‰“å°ç®€æ´è¯„ä¼°æŠ¥å‘Š"""
        comp = report_data['comparison']
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æ¨¡å‹å¯¹æ¯”è¯„ä¼°æŠ¥å‘Š")
        print(f"{'='*80}")
        print(f"ğŸ“ æ•°æ®é›†: {self.config.data_path}")
        print(f"ğŸ”„ æ ·æœ¬æ•°: {len(self.examples)}")
        print(f"ğŸŒ ç¿»è¯‘: {self.source_lang} â†’ {self.target_lang}")
        print(f"")
        
        print(f"ğŸ¯ è¯„ä¼°æŒ‡æ ‡å¯¹æ¯”:")
        print(f"{'æŒ‡æ ‡':<15} {'å¾®è°ƒå‰':<12} {'å¾®è°ƒå':<12} {'æ”¹è¿›å¹…åº¦':<15}")
        print(f"{'-'*60}")
        
        metrics = [
            ('ç¼–è¯‘é€šè¿‡ç‡', comp['compilation'], '%'),
            ('CodeBLEU', comp['codebleu'], ''),  
            ('ASTåŒ¹é…åº¦', comp['ast_match'], ''),
            ('DFGåŒ¹é…åº¦', comp['dfg_match'], '')
        ]
        
        for name, data, unit in metrics:
            before = data['before']
            after = data['after']
            improvement = data['improvement']
            
            if unit == '%':
                before_str = f"{before*100:.2f}%"
                after_str = f"{after*100:.2f}%"
            else:
                before_str = f"{before:.4f}"
                after_str = f"{after:.4f}"
            
            if improvement == float('inf'):
                improve_str = "âˆ"
            else:
                improve_str = f"{improvement:+.2f}%"
                
            print(f"{name:<15} {before_str:<12} {after_str:<12} {improve_str:<15}")
        
        print(f"\nğŸ† æ€»ä½“è¯„ä»·:")
        total_improvements = [
            comp['compilation']['improvement'],
            comp['codebleu']['improvement'], 
            comp['ast_match']['improvement'],
            comp['dfg_match']['improvement']
        ]
        
        positive_improvements = sum(1 for x in total_improvements if x > 0)
        avg_improvement = np.mean([x for x in total_improvements if x != float('inf')])
        
        print(f"   â€¢ æ”¹è¿›æŒ‡æ ‡æ•°: {positive_improvements}/4")
        print(f"   â€¢ å¹³å‡æ”¹è¿›å¹…åº¦: {avg_improvement:+.2f}%")
        
        # æ˜¾ç¤ºMcNemaræµ‹è¯•ç»“æœ
        if 'mcnemar_test' in report_data:
            mcnemar = report_data['mcnemar_test']
            print(f"")
            print(f"ğŸ“Š McNemarç¼–è¯‘é€šè¿‡ç‡æµ‹è¯•:")
            cm = mcnemar['confusion_matrix']
            print(f"   â€¢ n01 (å‰å¤±è´¥åæˆåŠŸ): {cm['n01']}")
            print(f"   â€¢ n10 (å‰æˆåŠŸåå¤±è´¥): {cm['n10']}")
            print(f"   â€¢ å‡€æ”¹è¿›: {cm['n01'] - cm['n10']}")
            print(f"   â€¢ ç»Ÿè®¡æ˜¾è‘—: {'âœ… æ˜¯' if mcnemar['is_significant'] else 'âŒ å¦'}")
            print(f"   â€¢ på€¼: {mcnemar['p_value']:.6f}")
        
        if positive_improvements >= 3:
            print(f"   â€¢ ğŸ‰ å¾®è°ƒæ•ˆæœæ˜¾è‘—ï¼Œæ¨¡å‹æ€§èƒ½å…¨é¢æå‡ï¼")
        elif positive_improvements >= 2:
            print(f"   â€¢ âœ… å¾®è°ƒæ•ˆæœè‰¯å¥½ï¼Œå¤šæ•°æŒ‡æ ‡æœ‰æ‰€æ”¹è¿›")
        elif positive_improvements >= 1:
            print(f"   â€¢ âš ï¸  å¾®è°ƒæ•ˆæœä¸€èˆ¬ï¼Œéƒ¨åˆ†æŒ‡æ ‡æœ‰æ”¹è¿›")
        else:
            print(f"   â€¢ âŒ å¾®è°ƒæ•ˆæœä¸ä½³ï¼Œå»ºè®®æ£€æŸ¥è®­ç»ƒé…ç½®")
            
        print(f"{'='*80}")
        
    def format_summary_report(self, report_data: Dict) -> str:
        """æ ¼å¼åŒ–ç®€æ´æŠ¥å‘Šä¸ºæ–‡æœ¬"""
        lines = []
        comp = report_data['comparison']
        
        lines.append("=" * 80)
        lines.append("æ¨¡å‹å¯¹æ¯”è¯„ä¼°æŠ¥å‘Š")
        lines.append("=" * 80)
        lines.append(f"æ•°æ®é›†: {self.config.data_path}")
        lines.append(f"æ ·æœ¬æ•°: {len(self.examples)}")
        lines.append(f"ç¿»è¯‘: {self.source_lang} â†’ {self.target_lang}")
        lines.append("")
        
        lines.append("è¯„ä¼°æŒ‡æ ‡å¯¹æ¯”:")
        lines.append(f"{'æŒ‡æ ‡':<15} {'å¾®è°ƒå‰':<12} {'å¾®è°ƒå':<12} {'æ”¹è¿›å¹…åº¦':<15}")
        lines.append("-" * 60)
        
        metrics = [
            ('ç¼–è¯‘é€šè¿‡ç‡', comp['compilation'], '%'),
            ('CodeBLEU', comp['codebleu'], ''),
            ('ASTåŒ¹é…åº¦', comp['ast_match'], ''),
            ('DFGåŒ¹é…åº¦', comp['dfg_match'], '')
        ]
        
        for name, data, unit in metrics:
            before = data['before']
            after = data['after']
            improvement = data['improvement']
            
            if unit == '%':
                before_str = f"{before*100:.2f}%"
                after_str = f"{after*100:.2f}%"
            else:
                before_str = f"{before:.4f}"
                after_str = f"{after:.4f}"
            
            if improvement == float('inf'):
                improve_str = "âˆ"
            else:
                improve_str = f"{improvement:+.2f}%"
                
            lines.append(f"{name:<15} {before_str:<12} {after_str:<12} {improve_str:<15}")
        
        return "\n".join(lines)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="æ¨¡å‹å¯¹æ¯”è¯„ä¼°è„šæœ¬")
    
    # å¿…éœ€å‚æ•°
    parser.add_argument("--model_before", required=True, type=str,
                       help="å¾®è°ƒå‰æ¨¡å‹è·¯å¾„")
    parser.add_argument("--model_after", required=True, type=str,  
                       help="å¾®è°ƒåæ¨¡å‹è·¯å¾„")
    
    # å¯é€‰å‚æ•°
    parser.add_argument("--data_path", default="data/qwen/Java-Python/val.jsonl", type=str,
                       help="éªŒè¯æ•°æ®é›†è·¯å¾„ (æ”¯æŒ: Java-Python, Java-C++, C++-Python)")
    parser.add_argument("--source_lang", default="java", type=str,
                       help="æºä»£ç è¯­è¨€ (ä¼šè‡ªåŠ¨ä»æ•°æ®è·¯å¾„æ¨æ–­)")
    parser.add_argument("--target_lang", default="python", type=str,
                       help="ç›®æ ‡ä»£ç è¯­è¨€ (ä¼šè‡ªåŠ¨ä»æ•°æ®è·¯å¾„æ¨æ–­)")
    parser.add_argument("--batch_size", default=8, type=int,
                       help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--max_target_length", default=400, type=int,
                       help="ç›®æ ‡ä»£ç æœ€å¤§é•¿åº¦")
    parser.add_argument("--output_dir", default="evaluation_results", type=str,
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu", type=str,
                       help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--no_save_predictions", action="store_true",
                       help="ä¸ä¿å­˜é¢„æµ‹ç»“æœ")
    parser.add_argument("--disable_mcnemar", action="store_true",
                       help="ç¦ç”¨McNemaræµ‹è¯•")
    parser.add_argument("--mcnemar_alpha", default=0.05, type=float,
                       help="McNemaræµ‹è¯•æ˜¾è‘—æ€§æ°´å¹³ (é»˜è®¤: 0.05)")
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # åˆ›å»ºè¯„ä¼°é…ç½®
    config = EvaluationConfig(
        model_before=args.model_before,
        model_after=args.model_after,
        data_path=args.data_path,
        source_lang=args.source_lang,
        target_lang=args.target_lang,
        batch_size=args.batch_size,
        max_target_length=args.max_target_length,
        output_dir=args.output_dir,
        device=args.device,
        save_predictions=not args.no_save_predictions,
        enable_mcnemar=not args.disable_mcnemar,
        mcnemar_alpha=args.mcnemar_alpha
    )
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelEvaluator(config)
    
    # è¿è¡Œè¯„ä¼°
    try:
        results = evaluator.run_evaluation()
        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {config.output_dir}")
        
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())