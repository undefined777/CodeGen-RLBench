#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿæµ‹è¯•ç”ŸæˆåŠŸèƒ½
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


def test_generation():
    """æµ‹è¯•æ¨¡å‹ç”ŸæˆåŠŸèƒ½"""
    print("ğŸš€ å¿«é€Ÿæµ‹è¯•æ¨¡å‹ç”ŸæˆåŠŸèƒ½")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    model_path = "/home/cxy/Qwen2.5-Coder/finetuning/sft/checkpoints/qwen0.5b-lr5e-5-wr10-wd0.0-bsz1024-maxlen1280"
    
    print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        padding_side='right'  # ä¸SFTè®­ç»ƒä¿æŒä¸€è‡´
    )
    
    # ç¡®ä¿æœ‰pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print(f"âœ… Tokenizeré…ç½®:")
    print(f"   Padding side: {tokenizer.padding_side}")
    print(f"   Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
    print(f"   EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # æµ‹è¯•ç”Ÿæˆ
    test_prompt = "Translate Java to C++:\n\n```java\nclass Test {\n    public static void main(String[] args) {\n        System.out.println(\"Hello\");\n    }\n}\n```"
    
    print(f"\nğŸ“ æµ‹è¯•æç¤º:")
    print(f"{test_prompt}")
    
    # ç¼–ç 
    inputs = tokenizer(
        test_prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=200
    )
    
    # ç§»åŠ¨åˆ°GPU
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    print(f"\nğŸ“Š è¾“å…¥ä¿¡æ¯:")
    print(f"   è¾“å…¥å½¢çŠ¶: {inputs['input_ids'].shape}")
    print(f"   Attention maskå½¢çŠ¶: {inputs['attention_mask'].shape}")
    print(f"   æœ‰æ•ˆé•¿åº¦: {inputs['attention_mask'].sum().item()}")
    
    # æ£€æŸ¥padding
    pad_positions = (inputs['input_ids'][0] == tokenizer.pad_token_id)
    mask_positions = (inputs['attention_mask'][0] == 0)
    
    print(f"   Padding tokens: {pad_positions.sum().item()}")
    print(f"   Mask zeros: {mask_positions.sum().item()}")
    
    if torch.equal(pad_positions, mask_positions):
        print(f"   âœ… Paddingä¸attention maskä¸€è‡´")
    else:
        print(f"   âŒ Paddingä¸attention maskä¸ä¸€è‡´ï¼")
    
    # ç”Ÿæˆ
    print(f"\nğŸ¤– å¼€å§‹ç”Ÿæˆ...")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # è§£ç 
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print(f"\nğŸ“ ç”Ÿæˆç»“æœ:")
    print(f"{generated_text}")
    
    # æ£€æŸ¥ç”Ÿæˆè´¨é‡
    print(f"\nğŸ” ç”Ÿæˆè´¨é‡åˆ†æ:")
    if "class" in generated_text.lower():
        print(f"   âœ… åŒ…å«classå…³é”®å­—")
    else:
        print(f"   âŒ ä¸åŒ…å«classå…³é”®å­—")
        
    if "main" in generated_text.lower():
        print(f"   âœ… åŒ…å«mainå…³é”®å­—")
    else:
        print(f"   âŒ ä¸åŒ…å«mainå…³é”®å­—")
        
    if "cout" in generated_text.lower() or "printf" in generated_text.lower():
        print(f"   âœ… åŒ…å«C++è¾“å‡ºè¯­å¥")
    else:
        print(f"   âŒ ä¸åŒ…å«C++è¾“å‡ºè¯­å¥")
    
    # æ£€æŸ¥ç”Ÿæˆé•¿åº¦
    original_length = inputs['input_ids'].shape[1]
    generated_length = outputs.shape[1]
    new_tokens = generated_length - original_length
    
    print(f"\nğŸ“Š ç”Ÿæˆé•¿åº¦åˆ†æ:")
    print(f"   åŸå§‹é•¿åº¦: {original_length} tokens")
    print(f"   ç”Ÿæˆåé•¿åº¦: {generated_length} tokens")
    print(f"   æ–°ç”Ÿæˆ: {new_tokens} tokens")
    
    if new_tokens > 1:
        print(f"   âœ… ç”Ÿæˆäº†æ–°å†…å®¹")
    else:
        print(f"   âŒ æ²¡æœ‰ç”Ÿæˆæ–°å†…å®¹")
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    test_generation() 